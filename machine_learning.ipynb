{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:30.448952300Z",
     "start_time": "2026-03-01T05:18:30.169621100Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "CHAINE DE MACHINE LEARNING\n",
    "Projet: Prediction du risque d'infarctus\n",
    "Dataset: dataset_infarctus.xlsx\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.046224500Z",
     "start_time": "2026-03-01T05:18:30.473101900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "1. CHARGEMENT DES DONNEES\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. CHARGEMENT ET EXPLORATION INITIALE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_excel('data/dataset_infarctus.xlsx')\n",
    "\n",
    "print(f\"\\nDimensions du dataset: {df.shape[0]} patients x {df.shape[1]} variables\")\n",
    "print(f\"\\nVariables disponibles:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n"
   ],
   "id": "1c12468f6964c781",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "1. CHARGEMENT ET EXPLORATION INITIALE\n",
      "================================================================================\n",
      "\n",
      "Dimensions du dataset: 1000 patients x 21 variables\n",
      "\n",
      "Variables disponibles:\n",
      "    1. age\n",
      "    2. sexe\n",
      "    3. imc\n",
      "    4. hypertension\n",
      "    5. diabete\n",
      "    6. hypercholesterolemie\n",
      "    7. antecedents_familiaux\n",
      "    8. tabagisme\n",
      "    9. sedentarite\n",
      "   10. pression_systolique\n",
      "   11. pression_diastolique\n",
      "   12. frequence_cardiaque\n",
      "   13. douleur_thoracique\n",
      "   14. dyspnee\n",
      "   15. cholesterol_total\n",
      "   16. ldl\n",
      "   17. hdl\n",
      "   18. triglycerides\n",
      "   19. glycemie\n",
      "   20. crp\n",
      "   21. infarctus\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.104674300Z",
     "start_time": "2026-03-01T05:18:31.068493800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "2. IDENTIFICATION DES TYPES DE VARIABLES\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. IDENTIFICATION DES TYPES DE VARIABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def identifier_types_variables(dataframe):\n",
    "    \"\"\"Identifie les variables binaires, catégorielles et numériques\"\"\"\n",
    "    variables_binaires = []\n",
    "    variables_categorielles = []\n",
    "    variables_numeriques = []\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        if col == 'infarctus':  # Exclure la variable cible\n",
    "            continue\n",
    "\n",
    "        valeurs_uniques = dataframe[col].dropna().unique()\n",
    "\n",
    "        # Variables binaires (0 et 1)\n",
    "        if len(valeurs_uniques) == 2 and set(valeurs_uniques).issubset({0, 1, 0.0, 1.0}):\n",
    "            variables_binaires.append(col)\n",
    "        # Variables catégorielles (type object ou nombre limité de valeurs uniques)\n",
    "        elif dataframe[col].dtype == 'object' or (len(valeurs_uniques) < 10 and dataframe[col].dtype in ['int64', 'float64']):\n",
    "            variables_categorielles.append(col)\n",
    "        # Variables numeriques continues\n",
    "        elif dataframe[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            variables_numeriques.append(col)\n",
    "\n",
    "    return variables_binaires, variables_categorielles, variables_numeriques\n",
    "\n",
    "variables_binaires, variables_categorielles, variables_numeriques = identifier_types_variables(df)\n",
    "\n",
    "print(f\"\\nVariables binaires ({len(variables_binaires)}):\")\n",
    "for i, var in enumerate(variables_binaires, 1):\n",
    "    count = df[var].value_counts()\n",
    "    print(f\"   {i:2d}. {var:30s} - 0: {count.get(0, 0):4d}, 1: {count.get(1, 0):4d}\")\n",
    "\n",
    "print(f\"\\nVariables categorielles ({len(variables_categorielles)}):\")\n",
    "for i, var in enumerate(variables_categorielles, 1):\n",
    "    valeurs_uniques = sorted(df[var].dropna().unique())\n",
    "    count = df[var].value_counts().sort_index()\n",
    "    print(f\"   {i:2d}. {var:30s} - Valeurs: {valeurs_uniques}\")\n",
    "    for val in valeurs_uniques:\n",
    "        print(f\"       Valeur {val}: {count.get(val, 0):4d} patients\")\n",
    "\n",
    "print(f\"\\nVariables numeriques continues ({len(variables_numeriques)}):\")\n",
    "for i, var in enumerate(variables_numeriques, 1):\n",
    "    min_val = df[var].min()\n",
    "    max_val = df[var].max()\n",
    "    mean_val = df[var].mean()\n",
    "    print(f\"   {i:2d}. {var:30s} - Min: {min_val:8.2f}, Max: {max_val:8.2f}, Moyenne: {mean_val:8.2f}\")\n",
    "\n",
    "# Separer les features de la variable cible\n",
    "if 'infarctus' in df.columns:\n",
    "    X = df.drop(columns=['infarctus'])\n",
    "    y = df['infarctus']\n",
    "else:\n",
    "    X = df.copy()\n",
    "    y = None\n"
   ],
   "id": "5a60ef5ffc7a86ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "2. IDENTIFICATION DES TYPES DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      "Variables binaires (8):\n",
      "    1. sexe                           - 0:  458, 1:  542\n",
      "    2. hypertension                   - 0:  381, 1:  619\n",
      "    3. diabete                        - 0:  532, 1:  468\n",
      "    4. hypercholesterolemie           - 0:  598, 1:  402\n",
      "    5. antecedents_familiaux          - 0:  728, 1:  272\n",
      "    6. sedentarite                    - 0:  506, 1:  494\n",
      "    7. douleur_thoracique             - 0:  651, 1:  349\n",
      "    8. dyspnee                        - 0:  733, 1:  267\n",
      "\n",
      "Variables categorielles (1):\n",
      "    1. tabagisme                      - Valeurs: [np.int64(0), np.int64(1), np.int64(2)]\n",
      "       Valeur 0:  525 patients\n",
      "       Valeur 1:  249 patients\n",
      "       Valeur 2:  226 patients\n",
      "\n",
      "Variables numeriques continues (11):\n",
      "    1. age                            - Min:     2.00, Max:   252.59, Moyenne:    55.32\n",
      "    2. imc                            - Min:    18.00, Max:    39.55, Moyenne:    27.07\n",
      "    3. pression_systolique            - Min:    93.12, Max:   429.74, Moyenne:   139.76\n",
      "    4. pression_diastolique           - Min:    59.83, Max:   112.15, Moyenne:    86.37\n",
      "    5. frequence_cardiaque            - Min:    45.00, Max:   298.56, Moyenne:    74.55\n",
      "    6. cholesterol_total              - Min:   130.43, Max:   304.24, Moyenne:   212.51\n",
      "    7. ldl                            - Min:    70.43, Max:   198.75, Moyenne:   128.34\n",
      "    8. hdl                            - Min:    20.00, Max:    82.36, Moyenne:    50.24\n",
      "    9. triglycerides                  - Min:    50.00, Max:   400.00, Moyenne:   169.11\n",
      "   10. glycemie                       - Min:    60.00, Max:   167.13, Moyenne:   109.18\n",
      "   11. crp                            - Min:     0.10, Max:    20.00, Moyenne:     3.84\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.145699600Z",
     "start_time": "2026-03-01T05:18:31.107666300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "3. GESTION DES OUTLIERS (VALEURS ABERRANTES)\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. GESTION DES OUTLIERS - METHODE IQR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# >>> PARAMETRE MODIFIABLE : Facteur IQR <<<\n",
    "FACTEUR_IQR = 2.5\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "print(f\"\\nConfiguration : FACTEUR_IQR = {FACTEUR_IQR}\")\n",
    "\n",
    "def detecter_outliers_iqr(dataframe, colonne, facteur_iqr):\n",
    "    Q1 = dataframe[colonne].quantile(0.25)\n",
    "    Q3 = dataframe[colonne].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    limite_inf = Q1 - facteur_iqr * IQR\n",
    "    limite_sup = Q3 + facteur_iqr * IQR\n",
    "\n",
    "    outliers = dataframe[(dataframe[colonne] < limite_inf) | (dataframe[colonne] > limite_sup)][colonne]\n",
    "\n",
    "    return {\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'limite_inf': limite_inf,\n",
    "        'limite_sup': limite_sup,\n",
    "        'n_outliers': len(outliers),\n",
    "        'pct_outliers': (len(outliers) / len(dataframe)) * 100,\n",
    "        'outliers_indices': outliers.index.tolist()\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANALYSE DES OUTLIERS PAR VARIABLE NUMERIQUE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# On analyse uniquement les variables numeriques continues (pas les binaires)\n",
    "variables_a_analyser = [var for var in variables_numeriques]\n",
    "\n",
    "print(f\"\\nNombre de variables a analyser : {len(variables_a_analyser)}\")\n",
    "\n",
    "# Dictionnaire pour stocker les resultats\n",
    "resultats_outliers = {}\n",
    "\n",
    "for var in variables_a_analyser:\n",
    "    stats = detecter_outliers_iqr(df, var, facteur_iqr=FACTEUR_IQR)\n",
    "    resultats_outliers[var] = stats\n",
    "\n",
    "    print(f\"\\n{var.upper()}\")\n",
    "    print(f\"   Q1                  : {stats['Q1']:>10.2f}\")\n",
    "    print(f\"   Q3                  : {stats['Q3']:>10.2f}\")\n",
    "    print(f\"   IQR                 : {stats['IQR']:>10.2f}\")\n",
    "    print(f\"   Limite inferieure   : {stats['limite_inf']:>10.2f}\")\n",
    "    print(f\"   Limite superieure   : {stats['limite_sup']:>10.2f}\")\n",
    "    print(f\"   Outliers detectes   : {stats['n_outliers']:>10d} ({stats['pct_outliers']:>6.2f}%)\")\n",
    "\n",
    "    if stats['n_outliers'] > 0:\n",
    "        print(f\"   --> ATTENTION : Valeurs aberrantes detectees\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TRAITEMENT : REMPLACEMENT DES OUTLIERS PAR NaN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Creer une copie pour le traitement\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Compter le nombre total de valeurs aberrantes remplacees\n",
    "total_outliers_remplaces = 0\n",
    "details_remplacement = []\n",
    "\n",
    "for var in variables_a_analyser:\n",
    "    stats = resultats_outliers[var]\n",
    "\n",
    "    if stats['n_outliers'] > 0:\n",
    "        # Identifier les outliers\n",
    "        condition_outliers = ((df_cleaned[var] < stats['limite_inf']) |\n",
    "                             (df_cleaned[var] > stats['limite_sup']))\n",
    "\n",
    "        n_avant = df_cleaned[var].notna().sum()\n",
    "\n",
    "        # Remplacer les outliers par NaN\n",
    "        df_cleaned.loc[condition_outliers, var] = np.nan\n",
    "\n",
    "        n_apres = df_cleaned[var].notna().sum()\n",
    "        n_remplaces = n_avant - n_apres\n",
    "        total_outliers_remplaces += n_remplaces\n",
    "\n",
    "        details_remplacement.append({\n",
    "            'Variable': var,\n",
    "            'Outliers remplaces': n_remplaces,\n",
    "            'Pourcentage': (n_remplaces / len(df_cleaned)) * 100\n",
    "        })\n",
    "\n",
    "        print(f\"\\n{var:30s} : {n_remplaces:4d} valeurs remplacees ({(n_remplaces / len(df_cleaned)) * 100:5.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'-' * 80}\")\n",
    "print(f\"TOTAL : {total_outliers_remplaces} valeurs aberrantes remplacees par NaN\")\n",
    "print(f\"{'-' * 80}\")"
   ],
   "id": "71996ee9d47961e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "3. GESTION DES OUTLIERS - METHODE IQR\n",
      "================================================================================\n",
      "\n",
      "Configuration : FACTEUR_IQR = 2.5\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANALYSE DES OUTLIERS PAR VARIABLE NUMERIQUE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Nombre de variables a analyser : 11\n",
      "\n",
      "AGE\n",
      "   Q1                  :      48.49\n",
      "   Q3                  :      61.48\n",
      "   IQR                 :      12.99\n",
      "   Limite inferieure   :      16.02\n",
      "   Limite superieure   :      93.95\n",
      "   Outliers detectes   :          2 (  0.20%)\n",
      "   --> ATTENTION : Valeurs aberrantes detectees\n",
      "\n",
      "IMC\n",
      "   Q1                  :      24.43\n",
      "   Q3                  :      29.66\n",
      "   IQR                 :       5.23\n",
      "   Limite inferieure   :      11.34\n",
      "   Limite superieure   :      42.75\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "PRESSION_SYSTOLIQUE\n",
      "   Q1                  :     130.69\n",
      "   Q3                  :     148.42\n",
      "   IQR                 :      17.74\n",
      "   Limite inferieure   :      86.35\n",
      "   Limite superieure   :     192.76\n",
      "   Outliers detectes   :          1 (  0.10%)\n",
      "   --> ATTENTION : Valeurs aberrantes detectees\n",
      "\n",
      "PRESSION_DIASTOLIQUE\n",
      "   Q1                  :      79.96\n",
      "   Q3                  :      92.77\n",
      "   IQR                 :      12.81\n",
      "   Limite inferieure   :      47.93\n",
      "   Limite superieure   :     124.81\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "FREQUENCE_CARDIAQUE\n",
      "   Q1                  :      67.37\n",
      "   Q3                  :      81.34\n",
      "   IQR                 :      13.97\n",
      "   Limite inferieure   :      32.45\n",
      "   Limite superieure   :     116.25\n",
      "   Outliers detectes   :          1 (  0.10%)\n",
      "   --> ATTENTION : Valeurs aberrantes detectees\n",
      "\n",
      "CHOLESTEROL_TOTAL\n",
      "   Q1                  :     193.68\n",
      "   Q3                  :     233.13\n",
      "   IQR                 :      39.45\n",
      "   Limite inferieure   :      95.07\n",
      "   Limite superieure   :     331.75\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "LDL\n",
      "   Q1                  :     113.14\n",
      "   Q3                  :     142.50\n",
      "   IQR                 :      29.36\n",
      "   Limite inferieure   :      39.73\n",
      "   Limite superieure   :     215.91\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "HDL\n",
      "   Q1                  :      42.21\n",
      "   Q3                  :      58.30\n",
      "   IQR                 :      16.09\n",
      "   Limite inferieure   :       1.98\n",
      "   Limite superieure   :      98.52\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "TRIGLYCERIDES\n",
      "   Q1                  :     133.38\n",
      "   Q3                  :     202.85\n",
      "   IQR                 :      69.47\n",
      "   Limite inferieure   :     -40.31\n",
      "   Limite superieure   :     376.53\n",
      "   Outliers detectes   :          1 (  0.10%)\n",
      "   --> ATTENTION : Valeurs aberrantes detectees\n",
      "\n",
      "GLYCEMIE\n",
      "   Q1                  :      89.63\n",
      "   Q3                  :     128.94\n",
      "   IQR                 :      39.32\n",
      "   Limite inferieure   :      -8.67\n",
      "   Limite superieure   :     227.23\n",
      "   Outliers detectes   :          0 (  0.00%)\n",
      "\n",
      "CRP\n",
      "   Q1                  :       0.99\n",
      "   Q3                  :       5.26\n",
      "   IQR                 :       4.28\n",
      "   Limite inferieure   :      -9.70\n",
      "   Limite superieure   :      15.95\n",
      "   Outliers detectes   :         19 (  1.90%)\n",
      "   --> ATTENTION : Valeurs aberrantes detectees\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAITEMENT : REMPLACEMENT DES OUTLIERS PAR NaN\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "age                            :    2 valeurs remplacees ( 0.20%)\n",
      "\n",
      "pression_systolique            :    1 valeurs remplacees ( 0.10%)\n",
      "\n",
      "frequence_cardiaque            :    1 valeurs remplacees ( 0.10%)\n",
      "\n",
      "triglycerides                  :    1 valeurs remplacees ( 0.10%)\n",
      "\n",
      "crp                            :   19 valeurs remplacees ( 1.90%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL : 24 valeurs aberrantes remplacees par NaN\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.187399500Z",
     "start_time": "2026-03-01T05:18:31.156701400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "4. PRETRAITEMENT - SPLIT TRAIN/TEST\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. PRETRAITEMENT - SPLIT TRAIN/TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Mise a jour du dataframe apres traitement des outliers\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "# Separation features et cible\n",
    "X = df.drop(columns=['infarctus'])\n",
    "y = df['infarctus']\n",
    "\n",
    "print(f\"\\nDataset complet :\")\n",
    "print(f\"   Features (X)     : {X.shape[0]} lignes x {X.shape[1]} colonnes\")\n",
    "print(f\"   Cible (y)        : {y.shape[0]} valeurs\")\n",
    "print(f\"   Distribution y   : {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Split train/test avec stratification (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 80)\n",
    "print(\"SPLIT TRAIN/TEST (80/20)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nEnsemble d'entrainement :\")\n",
    "print(f\"   X_train                              : {X_train.shape[0]} lignes x {X_train.shape[1]} colonnes\")\n",
    "print(f\"   y_train                              : {y_train.shape[0]} valeurs\")\n",
    "print(f\"   Distribution de la variable cible    : {y_train.value_counts().to_dict()}\")\n",
    "print(f\"   Proportion                           : {(len(X_train) / len(X)) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nEnsemble de test :\")\n",
    "print(f\"   X_test                               : {X_test.shape[0]} lignes x {X_test.shape[1]} colonnes\")\n",
    "print(f\"   y_test                               : {y_test.shape[0]} valeurs\")\n",
    "print(f\"   Distribution de la variable cible    : {y_test.value_counts().to_dict()}\")\n",
    "print(f\"   Proportion                           : {(len(X_test) / len(X)) * 100:.1f}%\")\n",
    "\n",
    "# Verification de la stratification\n",
    "print(f\"\\n\" + \"-\" * 80)\n",
    "print(\"VERIFICATION DE LA STRATIFICATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "prop_train = y_train.value_counts(normalize=True).sort_index()\n",
    "prop_test = y_test.value_counts(normalize=True).sort_index()\n",
    "prop_total = y.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\nProportions de la classe positive (infarctus=1) :\")\n",
    "print(f\"   Dataset complet  : {prop_total[1]:.4f}\")\n",
    "print(f\"   Train            : {prop_train[1]:.4f}\")\n",
    "print(f\"   Test             : {prop_test[1]:.4f}\")\n",
    "print(f\"   Difference       : {abs(prop_train[1] - prop_test[1]):.4f}\")\n",
    "\n",
    "if abs(prop_train[1] - prop_test[1]) < 0.01:\n",
    "    print(\"\\n   Stratification : OK - Distributions equilibrees\")\n",
    "else:\n",
    "    print(\"\\n   Stratification : ATTENTION - Leger desequilibre\")\n"
   ],
   "id": "bfd008c5f550c450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "4. PRETRAITEMENT - SPLIT TRAIN/TEST\n",
      "================================================================================\n",
      "\n",
      "Dataset complet :\n",
      "   Features (X)     : 1000 lignes x 20 colonnes\n",
      "   Cible (y)        : 1000 valeurs\n",
      "   Distribution y   : {0: 750, 1: 250}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SPLIT TRAIN/TEST (80/20)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ensemble d'entrainement :\n",
      "   X_train                              : 800 lignes x 20 colonnes\n",
      "   y_train                              : 800 valeurs\n",
      "   Distribution de la variable cible    : {0: 600, 1: 200}\n",
      "   Proportion                           : 80.0%\n",
      "\n",
      "Ensemble de test :\n",
      "   X_test                               : 200 lignes x 20 colonnes\n",
      "   y_test                               : 200 valeurs\n",
      "   Distribution de la variable cible    : {0: 150, 1: 50}\n",
      "   Proportion                           : 20.0%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION DE LA STRATIFICATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Proportions de la classe positive (infarctus=1) :\n",
      "   Dataset complet  : 0.2500\n",
      "   Train            : 0.2500\n",
      "   Test             : 0.2500\n",
      "   Difference       : 0.0000\n",
      "\n",
      "   Stratification : OK - Distributions equilibrees\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.225564300Z",
     "start_time": "2026-03-01T05:18:31.190392100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "5. IMPUTATION DES VALEURS MANQUANTES\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. IMPUTATION DES VALEURS MANQUANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\\nMETHODE : Imputation par la MEDIANE\"\"\")\n",
    "\n",
    "# Identifier les variables numeriques a imputer (dans train OU test)\n",
    "variables_a_imputer = [var for var in variables_numeriques\n",
    "                       if X_train[var].isnull().sum() > 0 or X_test[var].isnull().sum() > 0]\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 80)\n",
    "print(\"ANALYSE DES VALEURS MANQUANTES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nEnsemble d'entrainement (X_train) :\")\n",
    "missing_train = X_train.isnull().sum()\n",
    "missing_train = missing_train[missing_train > 0]\n",
    "if len(missing_train) > 0:\n",
    "    for var in missing_train.index:\n",
    "        print(f\"   {var:30s} : {missing_train[var]:4d} NaN ({(missing_train[var]/len(X_train))*100:5.2f}%)\")\n",
    "else:\n",
    "    print(\"   Aucune valeur manquante\")\n",
    "\n",
    "print(f\"\\nEnsemble de test (X_test) :\")\n",
    "missing_test = X_test.isnull().sum()\n",
    "missing_test = missing_test[missing_test > 0]\n",
    "if len(missing_test) > 0:\n",
    "    for var in missing_test.index:\n",
    "        print(f\"   {var:30s} : {missing_test[var]:4d} NaN ({(missing_test[var]/len(X_test))*100:5.2f}%)\")\n",
    "else:\n",
    "    print(\"   Aucune valeur manquante\")\n",
    "\n",
    "# Imputation\n",
    "if len(variables_a_imputer) > 0:\n",
    "    print(f\"\\n\" + \"-\" * 80)\n",
    "    print(\"APPLICATION DE L'IMPUTATION\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\nVariables a imputer : {len(variables_a_imputer)}\")\n",
    "\n",
    "    # Creer l'imputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "    # Fit sur le train uniquement\n",
    "    imputer.fit(X_train[variables_a_imputer])\n",
    "\n",
    "    # Stocker les medianes\n",
    "    medianes_train = {}\n",
    "    for idx, var in enumerate(variables_a_imputer):\n",
    "        medianes_train[var] = imputer.statistics_[idx]\n",
    "\n",
    "    # Transform train et test\n",
    "    X_train[variables_a_imputer] = imputer.transform(X_train[variables_a_imputer])\n",
    "    X_test[variables_a_imputer] = imputer.transform(X_test[variables_a_imputer])\n",
    "\n",
    "    print(f\"\\nMedianes utilisees (calculees sur train) :\")\n",
    "    for var, mediane in medianes_train.items():\n",
    "        print(f\"   {var:30s} : {mediane:10.2f}\")\n",
    "\n",
    "    print(f\"\\n\" + \"-\" * 80)\n",
    "    print(\"VERIFICATION POST-IMPUTATION\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\nX_train - Valeurs manquantes : {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"X_test  - Valeurs manquantes : {X_test.isnull().sum().sum()}\")\n",
    "\n",
    "    if X_train.isnull().sum().sum() == 0 and X_test.isnull().sum().sum() == 0:\n",
    "        print(\"\\nSUCCES : Toutes les valeurs manquantes ont ete imputees\")\n",
    "    else:\n",
    "        print(\"\\nATTENTION : Il reste des valeurs manquantes !\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nAucune imputation necessaire\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SPLIT ET IMPUTATION TERMINES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Mise a jour de la liste des variables numeriques apres pretraitement\n",
    "print(\"\\nOn met a jour la liste des variables numeriques avec les nouvelles valeurs\")\n",
    "\n",
    "# Reconstruction du dataframe complet pour re-analyser les types de variables numeriques\n",
    "df_final = pd.concat([X_train, X_test]).sort_index()\n",
    "df_final['infarctus'] = pd.concat([y_train, y_test]).sort_index()\n",
    "_, _, variables_numeriques = identifier_types_variables(df_final)"
   ],
   "id": "36bd9e0b7fe78141",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "5. IMPUTATION DES VALEURS MANQUANTES\n",
      "================================================================================\n",
      "\n",
      "METHODE : Imputation par la MEDIANE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANALYSE DES VALEURS MANQUANTES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ensemble d'entrainement (X_train) :\n",
      "   age                            :    2 NaN ( 0.25%)\n",
      "   pression_systolique            :    1 NaN ( 0.12%)\n",
      "   frequence_cardiaque            :    1 NaN ( 0.12%)\n",
      "   triglycerides                  :    1 NaN ( 0.12%)\n",
      "   crp                            :   19 NaN ( 2.38%)\n",
      "\n",
      "Ensemble de test (X_test) :\n",
      "   Aucune valeur manquante\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "APPLICATION DE L'IMPUTATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variables a imputer : 5\n",
      "\n",
      "Medianes utilisees (calculees sur train) :\n",
      "   age                            :      55.16\n",
      "   pression_systolique            :     140.06\n",
      "   frequence_cardiaque            :      74.25\n",
      "   triglycerides                  :     168.82\n",
      "   crp                            :       2.36\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERIFICATION POST-IMPUTATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "X_train - Valeurs manquantes : 0\n",
      "X_test  - Valeurs manquantes : 0\n",
      "\n",
      "SUCCES : Toutes les valeurs manquantes ont ete imputees\n",
      "\n",
      "================================================================================\n",
      "SPLIT ET IMPUTATION TERMINES\n",
      "================================================================================\n",
      "\n",
      "On met a jour la liste des variables numeriques avec les nouvelles valeurs\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.298369200Z",
     "start_time": "2026-03-01T05:18:31.235537600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "6. PREPARATION DES DONNEES POUR L'APPRENTISSAGE\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6. PREPARATION DES DONNEES POUR L'APPRENTISSAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"A. STANDARDISATION DES VARIABLES NUMERIQUES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "METHODE : Standardisation (moyenne=0, ecart-type=1)\n",
    "----------------------------------------------------\n",
    "- Fit sur les donnees d'ENTRAINEMENT uniquement\n",
    "- Transform sur train et test avec les parametres du train\n",
    "- Evite le data leakage\n",
    "\"\"\")\n",
    "\n",
    "# Creer le scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"FIT SUR L'ENSEMBLE D'ENTRAINEMENT\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Variables numeriques du train\n",
    "X_train_num = X_train[variables_numeriques].copy()\n",
    "\n",
    "print(f\"\\nVariables numeriques a standardiser : {len(variables_numeriques)}\")\n",
    "print(f\"Shape X_train numeriques : {X_train_num.shape}\")\n",
    "\n",
    "# Fit sur le train uniquement\n",
    "scaler.fit(X_train_num)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TRANSFORM SUR TRAIN ET TEST\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Transform train\n",
    "X_train_num_scaled = scaler.transform(X_train_num)\n",
    "X_train_num_scaled_df = pd.DataFrame(\n",
    "    X_train_num_scaled,\n",
    "    columns=variables_numeriques,\n",
    "    index=X_train_num.index\n",
    ")\n",
    "\n",
    "print(f\"\\nTRAIN apres standardisation :\")\n",
    "print(f\"   Shape                        : {X_train_num_scaled_df.shape}\")\n",
    "print(f\"   Moyenne (devrait etre 0)     : {X_train_num_scaled.mean():.6f}\")\n",
    "print(f\"   Ecart-type (devrait etre 1)  : {X_train_num_scaled.std():.6f}\")\n",
    "\n",
    "# Transform test avec les parametres du train\n",
    "X_test_num = X_test[variables_numeriques].copy()\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "X_test_num_scaled_df = pd.DataFrame(\n",
    "    X_test_num_scaled,\n",
    "    columns=variables_numeriques,\n",
    "    index=X_test_num.index\n",
    ")\n",
    "\n",
    "print(f\"\\nTEST apres standardisation :\")\n",
    "print(f\"   Shape                      : {X_test_num_scaled_df.shape}\")\n",
    "print(f\"   Moyenne                    : {X_test_num_scaled.mean():.6f}\")\n",
    "print(f\"   Ecart-type                 : {X_test_num_scaled.std():.6f}\")\n",
    "print(f\"   (Peut differer de 0 et 1 car parametres du train appliques)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"STANDARDISATION TERMINEE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Encoder les variables categorielles si elles existent (SANS standardisation)\n",
    "if len(variables_categorielles) > 0:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"B. ENCODAGE DES VARIABLES CATEGORIELLES\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\\nVariables categorielles a encoder : {len(variables_categorielles)}\")\n",
    "    print(\"Methode : One-Hot Encoding avec drop='first' (evite la multicollinearite)\")\n",
    "\n",
    "    # Creer l'encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "    # Variables categorielles du train\n",
    "    X_train_cat = X_train[variables_categorielles].copy()\n",
    "\n",
    "    print(f\"\\nShape X_train categorielles : {X_train_cat.shape}\")\n",
    "\n",
    "    # Fit sur le train uniquement\n",
    "    encoder.fit(X_train_cat)\n",
    "\n",
    "    print(\"\\nCategories detectees (fit sur train) :\")\n",
    "    for i, var in enumerate(variables_categorielles):\n",
    "        print(f\"   {var:30s} : {list(encoder.categories_[i])}\")\n",
    "\n",
    "    # Transform train\n",
    "    X_train_cat_encoded = encoder.transform(X_train_cat)\n",
    "\n",
    "    # Creer les noms de colonnes pour les variables encodees\n",
    "    encoded_feature_names = []\n",
    "    for i, var in enumerate(variables_categorielles):\n",
    "        categories = encoder.categories_[i][1:]  # Exclure la premiere categorie (dropped)\n",
    "        for cat in categories:\n",
    "            encoded_feature_names.append(f\"{var}_{cat}\")\n",
    "\n",
    "    X_train_cat_df = pd.DataFrame(\n",
    "        X_train_cat_encoded,\n",
    "        columns=encoded_feature_names,\n",
    "        index=X_train_cat.index\n",
    "    )\n",
    "\n",
    "    # Transform test avec les parametres du train\n",
    "    X_test_cat = X_test[variables_categorielles].copy()\n",
    "    X_test_cat_encoded = encoder.transform(X_test_cat)\n",
    "\n",
    "    X_test_cat_df = pd.DataFrame(\n",
    "        X_test_cat_encoded,\n",
    "        columns=encoded_feature_names,\n",
    "        index=X_test_cat.index\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ENCODAGE TERMINE\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Donnees encodees creees :\n",
    "   - X_train_cat_df : {X_train_cat_df.shape}\n",
    "   - X_test_cat_df  : {X_test_cat_df.shape}\n",
    "   - Variables restent binaires (0 ou 1)\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"COMBINAISON DES VARIABLES NUMERIQUES STANDARDISEES ET CATEGORIELLES ENCODEES\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Combiner variables numeriques STANDARDISEES et categorielles encodees NON STANDARDISEES pour chaque set de données\n",
    "\n",
    "    X_train_combined = pd.concat([X_train_num_scaled_df, X_train_cat_df], axis=1)\n",
    "    X_test_combined = pd.concat([X_test_num_scaled_df, X_test_cat_df], axis=1)\n",
    "\n",
    "    print(f\"\"\"\n",
    "Donnees combinees creees :\n",
    "   - X_train_combined : {X_train_combined.shape}\n",
    "   - X_test_combined  : {X_test_combined.shape}\n",
    "    \"\"\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"PAS DE VARIABLES CATEGORIELLES\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    print(\"\\nAucune variable categorielle a encoder\")\n",
    "    print(\"Utilisation uniquement des variables numeriques standardisees\")\n",
    "\n",
    "    X_train_combined = X_train_num_scaled_df.copy()\n",
    "    X_test_combined = X_test_num_scaled_df.copy()\n",
    "\n",
    "    print(f\"\\nX_train_combined : {X_train_combined.shape}\")\n",
    "    print(f\"X_test_combined  : {X_test_combined.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"C. GESTION DU DESEQUILIBRE DES CLASSES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculer les proportions des classes\n",
    "n_classe_0 = (y_train == 0).sum()\n",
    "n_classe_1 = (y_train == 1).sum()\n",
    "prop_classe_0 = n_classe_0 / len(y_train) * 100\n",
    "prop_classe_1 = n_classe_1 / len(y_train) * 100\n",
    "\n",
    "print(\"\\nAnalyse du desequilibre :\")\n",
    "print(f\"   Classe 0 (pas d'infarctus) : {n_classe_0} echantillons ({prop_classe_0:.2f}%)\")\n",
    "print(f\"   Classe 1 (infarctus)       : {n_classe_1} echantillons ({prop_classe_1:.2f}%)\")\n",
    "print(f\"   Ratio de desequilibre      : {n_classe_0 / n_classe_1:.2f}:1\")\n",
    "\n",
    "# Determiner si on doit utiliser class_weight='balanced'\n",
    "# Condition : desequilibre entre 0/100 et 40/60 (ou 60/40)\n",
    "min_prop = min(prop_classe_0, prop_classe_1)\n",
    "max_prop = max(prop_classe_0, prop_classe_1)\n",
    "\n",
    "if min_prop >= 40.0 and max_prop <= 60.0:\n",
    "    # Les classes sont equilibrees (entre 40/60 et 60/40)\n",
    "    class_weight = None\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"DECISION : class_weight = None\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nLes classes sont suffisamment equilibrees (entre 40/60 et 60/40)\")\n",
    "    print(\"Aucune ponderation necessaire\")\n",
    "else:\n",
    "    # Les classes sont desequilibrees\n",
    "    class_weight = 'balanced'\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"DECISION : class_weight = 'balanced'\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nLes classes sont desequilibrees (en dehors de l'intervalle 40/60 - 60/40)\")\n",
    "    print(\"Utilisation de la ponderation automatique 'balanced'\")\n",
    "\n",
    "    # Calculer les poids pour information\n",
    "    class_weights_array = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights_array))\n",
    "\n",
    "    print(\"\\nPoids calcules pour equilibrer les classes :\")\n",
    "    print(f\"   Classe 0 : {class_weight_dict[0]:.4f}\")\n",
    "    print(f\"   Classe 1 : {class_weight_dict[1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"GESTION DU DESEQUILIBRE DES CLASSES TERMINEE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPARATION DES DONNEES TERMINEE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASETS FINAUX PRETS POUR LA MODELISATION :\n",
    "   - X_train_combined : {X_train_combined.shape[0]} lignes x {X_train_combined.shape[1]} features\n",
    "   - X_test_combined  : {X_test_combined.shape[0]} lignes x {X_test_combined.shape[1]} features\n",
    "   - y_train          : {y_train.shape[0]} valeurs\n",
    "   - y_test           : {y_test.shape[0]} valeurs\n",
    "\"\"\")"
   ],
   "id": "ee802d96ff03528d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "6. PREPARATION DES DONNEES POUR L'APPRENTISSAGE\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "A. STANDARDISATION DES VARIABLES NUMERIQUES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "METHODE : Standardisation (moyenne=0, ecart-type=1)\n",
      "----------------------------------------------------\n",
      "- Fit sur les donnees d'ENTRAINEMENT uniquement\n",
      "- Transform sur train et test avec les parametres du train\n",
      "- Evite le data leakage\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FIT SUR L'ENSEMBLE D'ENTRAINEMENT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variables numeriques a standardiser : 11\n",
      "Shape X_train numeriques : (800, 11)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRANSFORM SUR TRAIN ET TEST\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "TRAIN apres standardisation :\n",
      "   Shape                        : (800, 11)\n",
      "   Moyenne (devrait etre 0)     : -0.000000\n",
      "   Ecart-type (devrait etre 1)  : 1.000000\n",
      "\n",
      "TEST apres standardisation :\n",
      "   Shape                      : (200, 11)\n",
      "   Moyenne                    : 0.020930\n",
      "   Ecart-type                 : 1.023168\n",
      "   (Peut differer de 0 et 1 car parametres du train appliques)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STANDARDISATION TERMINEE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "B. ENCODAGE DES VARIABLES CATEGORIELLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variables categorielles a encoder : 1\n",
      "Methode : One-Hot Encoding avec drop='first' (evite la multicollinearite)\n",
      "\n",
      "Shape X_train categorielles : (800, 1)\n",
      "\n",
      "Categories detectees (fit sur train) :\n",
      "   tabagisme                      : [np.int64(0), np.int64(1), np.int64(2)]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENCODAGE TERMINE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Donnees encodees creees :\n",
      "   - X_train_cat_df : (800, 2)\n",
      "   - X_test_cat_df  : (200, 2)\n",
      "   - Variables restent binaires (0 ou 1)\n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMBINAISON DES VARIABLES NUMERIQUES STANDARDISEES ET CATEGORIELLES ENCODEES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Donnees combinees creees :\n",
      "   - X_train_combined : (800, 13)\n",
      "   - X_test_combined  : (200, 13)\n",
      "    \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "C. GESTION DU DESEQUILIBRE DES CLASSES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Analyse du desequilibre :\n",
      "   Classe 0 (pas d'infarctus) : 600 echantillons (75.00%)\n",
      "   Classe 1 (infarctus)       : 200 echantillons (25.00%)\n",
      "   Ratio de desequilibre      : 3.00:1\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DECISION : class_weight = 'balanced'\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Les classes sont desequilibrees (en dehors de l'intervalle 40/60 - 60/40)\n",
      "Utilisation de la ponderation automatique 'balanced'\n",
      "\n",
      "Poids calcules pour equilibrer les classes :\n",
      "   Classe 0 : 0.6667\n",
      "   Classe 1 : 2.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GESTION DU DESEQUILIBRE DES CLASSES TERMINEE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "PREPARATION DES DONNEES TERMINEE\n",
      "================================================================================\n",
      "\n",
      "DATASETS FINAUX PRETS POUR LA MODELISATION :\n",
      "   - X_train_combined : 800 lignes x 13 features\n",
      "   - X_test_combined  : 200 lignes x 13 features\n",
      "   - y_train          : 800 valeurs\n",
      "   - y_test           : 200 valeurs\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.770479Z",
     "start_time": "2026-03-01T05:18:31.308342100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "7. IMPORTS POUR LES MODELES DE MACHINE LEARNING\n",
    "\"\"\"\n",
    "\n",
    "# Modeles supervises\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Modeles non supervises\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import gower\n",
    "\n",
    "# Metriques et evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    silhouette_score, davies_bouldin_score\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"7. IMPORTS POUR LES MODELES DE MACHINE LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTous les imports necessaires ont ete charges avec succes\")"
   ],
   "id": "d34e00ef1d0a6c38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "7. IMPORTS POUR LES MODELES DE MACHINE LEARNING\n",
      "================================================================================\n",
      "\n",
      "Tous les imports necessaires ont ete charges avec succes\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:31.792736400Z",
     "start_time": "2026-03-01T05:18:31.772473800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "8. PREPARATION DES DIFFERENTES VERSIONS DES DONNEES\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"8. PREPARATION DES DIFFERENTES VERSIONS DES DONNEES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pour tester les differents modeles, nous preparons plusieurs versions des donnees :\n",
    "\n",
    "1. DONNEES COMPLETES (standardisees + encodees) :\n",
    "   - Pour : Regression Logistique, SVM, Random Forest, Gradient Boosting\n",
    "   - Variables : Toutes (numeriques standardisees + categorielles encodees)\n",
    "\n",
    "2. DONNEES NUMERIQUES SEULES (standardisees) :\n",
    "   - Pour : PCA, K-Means (version numerique)\n",
    "   - Variables : Uniquement numeriques\n",
    "\n",
    "3. DONNEES MIXTES (non standardisees, non encodees) :\n",
    "   - Pour : Distance de Gower + Clustering Hierarchique\n",
    "   - Variables : Toutes (format original apres imputation)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 1 : DONNEES COMPLETES (standardisees + encodees)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Utiliser les variables binaires aussi\n",
    "X_train_binaires = X_train[variables_binaires].copy()\n",
    "X_test_binaires = X_test[variables_binaires].copy()\n",
    "\n",
    "# Combiner numeriques standardisees + binaires + categorielles encodees\n",
    "if len(variables_categorielles) > 0:\n",
    "    X_train_scaled = pd.concat([X_train_num_scaled_df, X_train_binaires, X_train_cat_df], axis=1)\n",
    "    X_test_scaled = pd.concat([X_test_num_scaled_df, X_test_binaires, X_test_cat_df], axis=1)\n",
    "else:\n",
    "    X_train_scaled = pd.concat([X_train_num_scaled_df, X_train_binaires], axis=1)\n",
    "    X_test_scaled = pd.concat([X_test_num_scaled_df, X_test_binaires], axis=1)\n",
    "\n",
    "print(f\"\\nX_train_scaled : {X_train_scaled.shape[0]} lignes x {X_train_scaled.shape[1]} features\")\n",
    "print(f\"X_test_scaled  : {X_test_scaled.shape[0]} lignes x {X_test_scaled.shape[1]} features\")\n",
    "print(f\"   - Variables numeriques standardisees : {len(variables_numeriques)}\")\n",
    "print(f\"   - Variables binaires                 : {len(variables_binaires)}\")\n",
    "if len(variables_categorielles) > 0:\n",
    "    print(f\"   - Variables categorielles encodees   : {len(encoded_feature_names)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 2 : DONNEES NUMERIQUES SEULES (standardisees)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "X_train_num_only = X_train_num_scaled_df.copy()\n",
    "X_test_num_only = X_test_num_scaled_df.copy()\n",
    "\n",
    "print(f\"\\nX_train_num_only : {X_train_num_only.shape[0]} lignes x {X_train_num_only.shape[1]} features\")\n",
    "print(f\"X_test_num_only  : {X_test_num_only.shape[0]} lignes x {X_test_num_only.shape[1]} features\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 3 : DONNEES MIXTES (non standardisees, non encodees)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Reconstituer le dataframe complet sans la variable cible\n",
    "df_final_sans_cible = df_final.drop(columns=['infarctus'])\n",
    "\n",
    "print(f\"\\ndf_final_sans_cible : {df_final_sans_cible.shape[0]} lignes x {df_final_sans_cible.shape[1]} features\")\n",
    "print(f\"   - Variables numeriques   : {len(variables_numeriques)}\")\n",
    "print(f\"   - Variables binaires     : {len(variables_binaires)}\")\n",
    "print(f\"   - Variables categorielles: {len(variables_categorielles)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPARATION DES DONNEES TERMINEE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionnaire pour stocker les resultats des modeles\n",
    "resultats_modeles = {}"
   ],
   "id": "508c32c5b0275c02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "8. PREPARATION DES DIFFERENTES VERSIONS DES DONNEES\n",
      "================================================================================\n",
      "\n",
      "Pour tester les differents modeles, nous preparons plusieurs versions des donnees :\n",
      "\n",
      "1. DONNEES COMPLETES (standardisees + encodees) :\n",
      "   - Pour : Regression Logistique, SVM, Random Forest, Gradient Boosting\n",
      "   - Variables : Toutes (numeriques standardisees + categorielles encodees)\n",
      "\n",
      "2. DONNEES NUMERIQUES SEULES (standardisees) :\n",
      "   - Pour : PCA, K-Means (version numerique)\n",
      "   - Variables : Uniquement numeriques\n",
      "\n",
      "3. DONNEES MIXTES (non standardisees, non encodees) :\n",
      "   - Pour : Distance de Gower + Clustering Hierarchique\n",
      "   - Variables : Toutes (format original apres imputation)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 1 : DONNEES COMPLETES (standardisees + encodees)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "X_train_scaled : 800 lignes x 21 features\n",
      "X_test_scaled  : 200 lignes x 21 features\n",
      "   - Variables numeriques standardisees : 11\n",
      "   - Variables binaires                 : 8\n",
      "   - Variables categorielles encodees   : 2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 2 : DONNEES NUMERIQUES SEULES (standardisees)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "X_train_num_only : 800 lignes x 11 features\n",
      "X_test_num_only  : 200 lignes x 11 features\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 3 : DONNEES MIXTES (non standardisees, non encodees)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "df_final_sans_cible : 1000 lignes x 20 features\n",
      "   - Variables numeriques   : 11\n",
      "   - Variables binaires     : 8\n",
      "   - Variables categorielles: 1\n",
      "\n",
      "================================================================================\n",
      "PREPARATION DES DONNEES TERMINEE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T07:28:14.002556200Z",
     "start_time": "2026-03-01T07:28:13.937356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "9. MODELE SUPERVISE : REGRESSION LOGISTIQUE\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"9. MODELE SUPERVISE : REGRESSION LOGISTIQUE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Modele lineaire de classification binaire\n",
    "   - Interprete les coefficients pour comprendre l'impact de chaque variable\n",
    "   - Baseline simple et efficace\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
    "   - Raison : Sensible aux echelles, necessite standardisation\n",
    "\"\"\")\n",
    "\n",
    "# Creer le modele\n",
    "logistic_model = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    random_state=42,\n",
    "    class_weight=class_weight,\n",
    "    C=2\n",
    ")\n",
    "\n",
    "# Entrainer le modele\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREDICTIONS ET EVALUATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Predictions sur train et test\n",
    "y_train_pred_lr = logistic_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = logistic_model.predict(X_test_scaled)\n",
    "\n",
    "# Probabilites pour ROC-AUC\n",
    "y_train_proba_lr = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba_lr = logistic_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metriques sur l'ensemble d'entrainement\n",
    "acc_train_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "precision_train_lr = precision_score(y_train, y_train_pred_lr, zero_division=0)\n",
    "recall_train_lr = recall_score(y_train, y_train_pred_lr, zero_division=0)\n",
    "f1_train_lr = f1_score(y_train, y_train_pred_lr, zero_division=0)\n",
    "roc_auc_train_lr = roc_auc_score(y_train, y_train_proba_lr)\n",
    "\n",
    "# Metriques sur l'ensemble de test\n",
    "acc_test_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "precision_test_lr = precision_score(y_test, y_test_pred_lr, zero_division=0)\n",
    "recall_test_lr = recall_score(y_test, y_test_pred_lr, zero_division=0)\n",
    "f1_test_lr = f1_score(y_test, y_test_pred_lr, zero_division=0)\n",
    "roc_auc_test_lr = roc_auc_score(y_test, y_test_proba_lr)\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\")\n",
    "print(f\"   Precision globale   : {acc_train_lr*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_train_lr*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_train_lr*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_train_lr*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_train_lr*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE DE TEST :\")\n",
    "print(f\"   Precision globale   : {acc_test_lr*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_test_lr*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_test_lr*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_test_lr*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_test_lr*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MATRICE DE CONFUSION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "print(\"\\n\", cm_lr)\n",
    "print(f\"\\n   Vrais Negatifs  (TN) : {cm_lr[0, 0]}\")\n",
    "print(f\"   Faux Positifs   (FP) : {cm_lr[0, 1]}\")\n",
    "print(f\"   Faux Negatifs   (FN) : {cm_lr[1, 0]}\")\n",
    "print(f\"   Vrais Positifs  (TP) : {cm_lr[1, 1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RAPPORT DE CLASSIFICATION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\", classification_report(y_test, y_test_pred_lr, target_names=['Pas infarctus', 'Infarctus']))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Obtenir les coefficients (importance des variables)\n",
    "# Pour la regression logistique, on utilise la valeur absolue des coefficients\n",
    "# NORMALISATION : diviser par la somme pour etre comparable aux autres modeles\n",
    "coef_abs = np.abs(logistic_model.coef_[0])\n",
    "coef_normalized = coef_abs / coef_abs.sum()  # Normaliser pour que la somme = 1\n",
    "\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': coef_normalized\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\", feature_importance_lr.head(20).to_string(index=False))\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['Regression Logistique'] = {\n",
    "    'modele': logistic_model,\n",
    "    'accuracy_train': acc_train_lr,\n",
    "    'accuracy_test': acc_test_lr,\n",
    "    'precision_test': precision_test_lr,\n",
    "    'recall_test': recall_test_lr,\n",
    "    'f1_test': f1_test_lr,\n",
    "    'roc_auc_test': roc_auc_test_lr,\n",
    "    'predictions_test': y_test_pred_lr,\n",
    "    'probas_test': y_test_proba_lr,\n",
    "    'feature_importance': feature_importance_lr\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGRESSION LOGISTIQUE TERMINEE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "ce76bf24b8a240bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "9. MODELE SUPERVISE : REGRESSION LOGISTIQUE\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Modele lineaire de classification binaire\n",
      "   - Interprete les coefficients pour comprendre l'impact de chaque variable\n",
      "   - Baseline simple et efficace\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
      "   - Raison : Sensible aux echelles, necessite standardisation\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREDICTIONS ET EVALUATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\n",
      "   Precision globale   : 96.38%\n",
      "   Precision positive  : 88.00%\n",
      "   Recall              : 99.00%\n",
      "   F1-Score            : 93.18%\n",
      "   ROC-AUC             : 99.65%\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE DE TEST :\n",
      "   Precision globale   : 97.00%\n",
      "   Precision positive  : 89.29%\n",
      "   Recall              : 100.00%\n",
      "   F1-Score            : 94.34%\n",
      "   ROC-AUC             : 99.87%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MATRICE DE CONFUSION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " [[144   6]\n",
      " [  0  50]]\n",
      "\n",
      "   Vrais Negatifs  (TN) : 144\n",
      "   Faux Positifs   (FP) : 6\n",
      "   Faux Negatifs   (FN) : 0\n",
      "   Vrais Positifs  (TP) : 50\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RAPPORT DE CLASSIFICATION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Pas infarctus       1.00      0.96      0.98       150\n",
      "    Infarctus       0.89      1.00      0.94        50\n",
      "\n",
      "     accuracy                           0.97       200\n",
      "    macro avg       0.95      0.98      0.96       200\n",
      " weighted avg       0.97      0.97      0.97       200\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "               feature  importance\n",
      "              diabete    0.164424\n",
      "         hypertension    0.162368\n",
      "   douleur_thoracique    0.122520\n",
      "              dyspnee    0.112230\n",
      "                 sexe    0.091279\n",
      "          tabagisme_2    0.089227\n",
      " hypercholesterolemie    0.074055\n",
      "                  ldl    0.049758\n",
      "                  age    0.044048\n",
      "                  crp    0.021494\n",
      "             glycemie    0.018454\n",
      "  pression_systolique    0.010942\n",
      "    cholesterol_total    0.010860\n",
      "antecedents_familiaux    0.005576\n",
      "          tabagisme_1    0.004965\n",
      "          sedentarite    0.004622\n",
      "        triglycerides    0.004308\n",
      "                  hdl    0.002876\n",
      " pression_diastolique    0.002873\n",
      "  frequence_cardiaque    0.002039\n",
      "\n",
      "================================================================================\n",
      "REGRESSION LOGISTIQUE TERMINEE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T08:01:50.247303300Z",
     "start_time": "2026-03-01T08:01:50.177632100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "TEST DE PREDICTION SUR UNE LIGNE SPECIFIQUE\n",
    "\"\"\"\n",
    "\n",
    "# Paramètre : numéro de ligne à tester\n",
    "ligne_test = 67  # Modifier ce numéro pour tester une autre ligne\n",
    "\n",
    "# Gestion d'erreur : vérifier que le numéro de ligne existe\n",
    "nombre_lignes = len(X_test_scaled)\n",
    "\n",
    "if ligne_test < 0 or ligne_test >= nombre_lignes:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ERREUR : NUMERO DE LIGNE INVALIDE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nLe numéro de ligne {ligne_test} n'existe pas dans X_test_scaled.\")\n",
    "    print(f\"X_test_scaled contient {nombre_lignes} lignes (indices de 0 à {nombre_lignes - 1}).\")\n",
    "    print(f\"\\nVeuillez choisir un numéro de ligne entre 0 et {nombre_lignes - 1}.\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    # Extraire la ligne spécifique de X_test_scaled\n",
    "    ligne_extraite = X_test_scaled.iloc[[ligne_test]]\n",
    "\n",
    "    # Prédire les probabilités pour cette ligne\n",
    "    predict = logistic_model.predict(ligne_extraite)[0]\n",
    "    probas = logistic_model.predict_proba(ligne_extraite)[0]\n",
    "\n",
    "    # Arrondir au 2ème centième (2 décimales)\n",
    "    proba_classe_0 = round(probas[0], 10)\n",
    "    proba_classe_1 = round(probas[1], 10)\n",
    "\n",
    "    # Récupérer la valeur réelle de y_test pour cette ligne\n",
    "    valeur_reelle = y_test.iloc[ligne_test]\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PREDICTION POUR LA LIGNE {ligne_test} DE X_test\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPrédiction : {predict}\")\n",
    "    print(f\"\\nProbabilités prédites :\")\n",
    "    print(f\"   Classe 0 (pas d'infarctus) : {proba_classe_0:.2f}\")\n",
    "    print(f\"   Classe 1 (infarctus)       : {proba_classe_1:.2f}\")\n",
    "    print(f\"\\nValeur réelle (y_test) : {int(valeur_reelle)}\")\n",
    "    print(\"=\" * 80)"
   ],
   "id": "e7d83b13caac22c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREDICTION POUR LA LIGNE 67 DE X_test\n",
      "================================================================================\n",
      "\n",
      "Prédiction : 1\n",
      "\n",
      "Probabilités prédites :\n",
      "   Classe 0 (pas d'infarctus) : 0.30\n",
      "   Classe 1 (infarctus)       : 0.70\n",
      "\n",
      "Valeur réelle (y_test) : 1\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:32.217629800Z",
     "start_time": "2026-03-01T05:18:31.870773100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "10. MODELE SUPERVISE : RANDOM FOREST CLASSIFIER\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"10. MODELE SUPERVISE : RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Ensemble de arbres de decision\n",
    "   - Robuste, gere bien les interactions non-lineaires\n",
    "   - Peu sensible aux outliers et aux echelles\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
    "   - Note : Random Forest n'a pas besoin de standardisation, mais on utilise\n",
    "     les memes donnees pour comparer avec les autres modeles\n",
    "\"\"\")\n",
    "\n",
    "# Creer le modele\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrainer le modele\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREDICTIONS ET EVALUATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Predictions sur train et test\n",
    "y_train_pred_rf = rf_model.predict(X_train_scaled)\n",
    "y_test_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Probabilites pour ROC-AUC\n",
    "y_train_proba_rf = rf_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metriques sur l'ensemble d'entrainement\n",
    "acc_train_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "precision_train_rf = precision_score(y_train, y_train_pred_rf, zero_division=0)\n",
    "recall_train_rf = recall_score(y_train, y_train_pred_rf, zero_division=0)\n",
    "f1_train_rf = f1_score(y_train, y_train_pred_rf, zero_division=0)\n",
    "roc_auc_train_rf = roc_auc_score(y_train, y_train_proba_rf)\n",
    "\n",
    "# Metriques sur l'ensemble de test\n",
    "acc_test_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "precision_test_rf = precision_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "recall_test_rf = recall_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "f1_test_rf = f1_score(y_test, y_test_pred_rf, zero_division=0)\n",
    "roc_auc_test_rf = roc_auc_score(y_test, y_test_proba_rf)\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\")\n",
    "print(f\"   Precision globale   : {acc_train_rf*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_train_rf*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_train_rf*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_train_rf*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_train_rf*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE DE TEST :\")\n",
    "print(f\"   Precision globale   : {acc_test_rf*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_test_rf*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_test_rf*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_test_rf*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_test_rf*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MATRICE DE CONFUSION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "print(\"\\n\", cm_rf)\n",
    "print(f\"\\n   Vrais Negatifs  (TN) : {cm_rf[0, 0]}\")\n",
    "print(f\"   Faux Positifs   (FP) : {cm_rf[0, 1]}\")\n",
    "print(f\"   Faux Negatifs   (FN) : {cm_rf[1, 0]}\")\n",
    "print(f\"   Vrais Positifs  (TP) : {cm_rf[1, 1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RAPPORT DE CLASSIFICATION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\", classification_report(y_test, y_test_pred_rf, target_names=['Pas infarctus', 'Infarctus']))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Obtenir l'importance des variables\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\", feature_importance_rf.head(20).to_string(index=False))\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['Random Forest'] = {\n",
    "    'modele': rf_model,\n",
    "    'accuracy_train': acc_train_rf,\n",
    "    'accuracy_test': acc_test_rf,\n",
    "    'precision_test': precision_test_rf,\n",
    "    'recall_test': recall_test_rf,\n",
    "    'f1_test': f1_test_rf,\n",
    "    'roc_auc_test': roc_auc_test_rf,\n",
    "    'predictions_test': y_test_pred_rf,\n",
    "    'probas_test': y_test_proba_rf,\n",
    "    'feature_importance': feature_importance_rf\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANDOM FOREST CLASSIFIER TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "8a9a5e9c5a819151",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "10. MODELE SUPERVISE : RANDOM FOREST CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Ensemble de arbres de decision\n",
      "   - Robuste, gere bien les interactions non-lineaires\n",
      "   - Peu sensible aux outliers et aux echelles\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
      "   - Note : Random Forest n'a pas besoin de standardisation, mais on utilise\n",
      "     les memes donnees pour comparer avec les autres modeles\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREDICTIONS ET EVALUATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\n",
      "   Precision globale   : 93.62%\n",
      "   Precision positive  : 81.43%\n",
      "   Recall              : 96.50%\n",
      "   F1-Score            : 88.33%\n",
      "   ROC-AUC             : 98.90%\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE DE TEST :\n",
      "   Precision globale   : 89.50%\n",
      "   Precision positive  : 76.36%\n",
      "   Recall              : 84.00%\n",
      "   F1-Score            : 80.00%\n",
      "   ROC-AUC             : 96.25%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MATRICE DE CONFUSION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " [[137  13]\n",
      " [  8  42]]\n",
      "\n",
      "   Vrais Negatifs  (TN) : 137\n",
      "   Faux Positifs   (FP) : 13\n",
      "   Faux Negatifs   (FN) : 8\n",
      "   Vrais Positifs  (TP) : 42\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RAPPORT DE CLASSIFICATION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Pas infarctus       0.94      0.91      0.93       150\n",
      "    Infarctus       0.76      0.84      0.80        50\n",
      "\n",
      "     accuracy                           0.90       200\n",
      "    macro avg       0.85      0.88      0.86       200\n",
      " weighted avg       0.90      0.90      0.90       200\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "               feature  importance\n",
      "              diabete    0.151663\n",
      "         hypertension    0.149311\n",
      "             glycemie    0.098893\n",
      "                  ldl    0.085419\n",
      "                  age    0.083628\n",
      "    cholesterol_total    0.074541\n",
      "   douleur_thoracique    0.072952\n",
      " hypercholesterolemie    0.049647\n",
      "  pression_systolique    0.042944\n",
      "                  imc    0.037035\n",
      " pression_diastolique    0.031681\n",
      "                  crp    0.029595\n",
      "                  hdl    0.019146\n",
      "                 sexe    0.015808\n",
      "          tabagisme_2    0.015321\n",
      "              dyspnee    0.014837\n",
      "        triglycerides    0.014768\n",
      "  frequence_cardiaque    0.008952\n",
      "          sedentarite    0.001894\n",
      "antecedents_familiaux    0.001059\n",
      "\n",
      "================================================================================\n",
      "RANDOM FOREST CLASSIFIER TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T09:09:17.195653100Z",
     "start_time": "2026-03-01T09:09:16.654943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "11. MODELE SUPERVISE : GRADIENT BOOSTING CLASSIFIER\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"11. MODELE SUPERVISE : GRADIENT BOOSTING CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Ensemble d'arbres construits sequentiellement\n",
    "   - Chaque arbre corrige les erreurs des precedents\n",
    "   - Souvent tres performant sur donnees tabulaires\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ENTRAINEMENT DU MODELE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Creer le modele\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrainer le modele\n",
    "print(f\"\\nEntrainement sur {X_train_scaled.shape[0]} echantillons avec {X_train_scaled.shape[1]} features...\")\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "print(\"Entrainement termine\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREDICTIONS ET EVALUATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Predictions sur train et test\n",
    "y_train_pred_gb = gb_model.predict(X_train_scaled)\n",
    "y_test_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Probabilites pour ROC-AUC\n",
    "y_train_proba_gb = gb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba_gb = gb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metriques sur l'ensemble d'entrainement\n",
    "acc_train_gb = accuracy_score(y_train, y_train_pred_gb)\n",
    "precision_train_gb = precision_score(y_train, y_train_pred_gb, zero_division=0)\n",
    "recall_train_gb = recall_score(y_train, y_train_pred_gb, zero_division=0)\n",
    "f1_train_gb = f1_score(y_train, y_train_pred_gb, zero_division=0)\n",
    "roc_auc_train_gb = roc_auc_score(y_train, y_train_proba_gb)\n",
    "\n",
    "# Metriques sur l'ensemble de test\n",
    "acc_test_gb = accuracy_score(y_test, y_test_pred_gb)\n",
    "precision_test_gb = precision_score(y_test, y_test_pred_gb, zero_division=0)\n",
    "recall_test_gb = recall_score(y_test, y_test_pred_gb, zero_division=0)\n",
    "f1_test_gb = f1_score(y_test, y_test_pred_gb, zero_division=0)\n",
    "roc_auc_test_gb = roc_auc_score(y_test, y_test_proba_gb)\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\")\n",
    "print(f\"   Precision globale   : {acc_train_gb*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_train_gb*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_train_gb*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_train_gb*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_train_gb*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE DE TEST :\")\n",
    "print(f\"   Precision globale   : {acc_test_gb*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_test_gb*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_test_gb*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_test_gb*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_test_gb*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MATRICE DE CONFUSION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cm_gb = confusion_matrix(y_test, y_test_pred_gb)\n",
    "print(\"\\n\", cm_gb)\n",
    "print(f\"\\n   Vrais Negatifs  (TN) : {cm_gb[0, 0]}\")\n",
    "print(f\"   Faux Positifs   (FP) : {cm_gb[0, 1]}\")\n",
    "print(f\"   Faux Negatifs   (FN) : {cm_gb[1, 0]}\")\n",
    "print(f\"   Vrais Positifs  (TP) : {cm_gb[1, 1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RAPPORT DE CLASSIFICATION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\", classification_report(y_test, y_test_pred_gb, target_names=['Pas infarctus', 'Infarctus']))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Obtenir l'importance des variables\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\", feature_importance_gb.head(20).to_string(index=False))\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['Gradient Boosting'] = {\n",
    "    'modele': gb_model,\n",
    "    'accuracy_train': acc_train_gb,\n",
    "    'accuracy_test': acc_test_gb,\n",
    "    'precision_test': precision_test_gb,\n",
    "    'recall_test': recall_test_gb,\n",
    "    'f1_test': f1_test_gb,\n",
    "    'roc_auc_test': roc_auc_test_gb,\n",
    "    'predictions_test': y_test_pred_gb,\n",
    "    'probas_test': y_test_proba_gb,\n",
    "    'feature_importance': feature_importance_gb\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GRADIENT BOOSTING CLASSIFIER TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "4b69432cd06e65d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "11. MODELE SUPERVISE : GRADIENT BOOSTING CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Ensemble d'arbres construits sequentiellement\n",
      "   - Chaque arbre corrige les erreurs des precedents\n",
      "   - Souvent tres performant sur donnees tabulaires\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENTRAINEMENT DU MODELE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Entrainement sur 800 echantillons avec 21 features...\n",
      "Entrainement termine\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREDICTIONS ET EVALUATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\n",
      "   Precision globale   : 98.25%\n",
      "   Precision positive  : 99.47%\n",
      "   Recall              : 93.50%\n",
      "   F1-Score            : 96.39%\n",
      "   ROC-AUC             : 99.92%\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE DE TEST :\n",
      "   Precision globale   : 93.50%\n",
      "   Precision positive  : 97.44%\n",
      "   Recall              : 76.00%\n",
      "   F1-Score            : 85.39%\n",
      "   ROC-AUC             : 98.15%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MATRICE DE CONFUSION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " [[149   1]\n",
      " [ 12  38]]\n",
      "\n",
      "   Vrais Negatifs  (TN) : 149\n",
      "   Faux Positifs   (FP) : 1\n",
      "   Faux Negatifs   (FN) : 12\n",
      "   Vrais Positifs  (TP) : 38\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RAPPORT DE CLASSIFICATION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Pas infarctus       0.93      0.99      0.96       150\n",
      "    Infarctus       0.97      0.76      0.85        50\n",
      "\n",
      "     accuracy                           0.94       200\n",
      "    macro avg       0.95      0.88      0.91       200\n",
      " weighted avg       0.94      0.94      0.93       200\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "               feature  importance\n",
      "              diabete    0.196725\n",
      "         hypertension    0.182649\n",
      "   douleur_thoracique    0.101225\n",
      "                  ldl    0.089382\n",
      "    cholesterol_total    0.066490\n",
      "             glycemie    0.064655\n",
      " hypercholesterolemie    0.056421\n",
      "                  age    0.052498\n",
      "                 sexe    0.044710\n",
      "                  crp    0.040448\n",
      "              dyspnee    0.037450\n",
      "          tabagisme_2    0.034860\n",
      "  pression_systolique    0.013003\n",
      "                  imc    0.008934\n",
      " pression_diastolique    0.007557\n",
      "        triglycerides    0.001499\n",
      "  frequence_cardiaque    0.000936\n",
      "                  hdl    0.000560\n",
      "antecedents_familiaux    0.000000\n",
      "          sedentarite    0.000000\n",
      "\n",
      "================================================================================\n",
      "GRADIENT BOOSTING CLASSIFIER TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:36.410998800Z",
     "start_time": "2026-03-01T05:18:33.237761600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "12. MODELE SUPERVISE : SVM (SUPPORT VECTOR MACHINE)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"12. MODELE SUPERVISE : SVM (SUPPORT VECTOR MACHINE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Trouve l'hyperplan optimal separant les classes\n",
    "   - Efficace en haute dimension\n",
    "   - Tres sensible aux echelles (necessite standardisation)\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
    "   - Raison : SVM est tres sensible aux echelles\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ENTRAINEMENT DU MODELE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Creer le modele\n",
    "svm_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    random_state=42,\n",
    "    class_weight=class_weight,\n",
    "    probability=True\n",
    ")\n",
    "\n",
    "# Entrainer le modele\n",
    "print(f\"\\nEntrainement sur {X_train_scaled.shape[0]} echantillons avec {X_train_scaled.shape[1]} features...\")\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "print(\"Entrainement termine\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREDICTIONS ET EVALUATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Predictions sur train et test\n",
    "y_train_pred_svm = svm_model.predict(X_train_scaled)\n",
    "y_test_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Probabilites pour ROC-AUC\n",
    "y_train_proba_svm = svm_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Metriques sur l'ensemble d'entrainement\n",
    "acc_train_svm = accuracy_score(y_train, y_train_pred_svm)\n",
    "precision_train_svm = precision_score(y_train, y_train_pred_svm, zero_division=0)\n",
    "recall_train_svm = recall_score(y_train, y_train_pred_svm, zero_division=0)\n",
    "f1_train_svm = f1_score(y_train, y_train_pred_svm, zero_division=0)\n",
    "roc_auc_train_svm = roc_auc_score(y_train, y_train_proba_svm)\n",
    "\n",
    "# Metriques sur l'ensemble de test\n",
    "acc_test_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "precision_test_svm = precision_score(y_test, y_test_pred_svm, zero_division=0)\n",
    "recall_test_svm = recall_score(y_test, y_test_pred_svm, zero_division=0)\n",
    "f1_test_svm = f1_score(y_test, y_test_pred_svm, zero_division=0)\n",
    "roc_auc_test_svm = roc_auc_score(y_test, y_test_proba_svm)\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\")\n",
    "print(f\"   Precision globale   : {acc_train_svm*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_train_svm*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_train_svm*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_train_svm*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_train_svm*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPERFORMANCES SUR L'ENSEMBLE DE TEST :\")\n",
    "print(f\"   Precision globale   : {acc_test_svm*100:.2f}%\")\n",
    "print(f\"   Precision positive  : {precision_test_svm*100:.2f}%\")\n",
    "print(f\"   Recall              : {recall_test_svm*100:.2f}%\")\n",
    "print(f\"   F1-Score            : {f1_test_svm*100:.2f}%\")\n",
    "print(f\"   ROC-AUC             : {roc_auc_test_svm*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MATRICE DE CONFUSION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "cm_svm = confusion_matrix(y_test, y_test_pred_svm)\n",
    "print(\"\\n\", cm_svm)\n",
    "print(f\"\\n   Vrais Negatifs  (TN) : {cm_svm[0, 0]}\")\n",
    "print(f\"   Faux Positifs   (FP) : {cm_svm[0, 1]}\")\n",
    "print(f\"   Faux Negatifs   (FN) : {cm_svm[1, 0]}\")\n",
    "print(f\"   Vrais Positifs  (TP) : {cm_svm[1, 1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"RAPPORT DE CLASSIFICATION (TEST)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\", classification_report(y_test, y_test_pred_svm, target_names=['Pas infarctus', 'Infarctus']))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pour SVM avec kernel RBF, on utilise la permutation importance\n",
    "# Methode : mesurer l'impact de chaque variable en la permutant\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "print(\"\\nCalcul de l'importance par permutation (peut prendre quelques secondes)...\")\n",
    "perm_importance = permutation_importance(\n",
    "    svm_model, X_test_scaled, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "feature_importance_svm = pd.DataFrame({\n",
    "    'feature': X_train_scaled.columns,\n",
    "    'importance': perm_importance.importances_mean\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\", feature_importance_svm.head(20).to_string(index=False))\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['SVM'] = {\n",
    "    'modele': svm_model,\n",
    "    'accuracy_train': acc_train_svm,\n",
    "    'accuracy_test': acc_test_svm,\n",
    "    'precision_test': precision_test_svm,\n",
    "    'recall_test': recall_test_svm,\n",
    "    'f1_test': f1_test_svm,\n",
    "    'roc_auc_test': roc_auc_test_svm,\n",
    "    'predictions_test': y_test_pred_svm,\n",
    "    'probas_test': y_test_proba_svm,\n",
    "    'feature_importance': feature_importance_svm\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SVM TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "2e75b65baf527b90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "12. MODELE SUPERVISE : SVM (SUPPORT VECTOR MACHINE)\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Trouve l'hyperplan optimal separant les classes\n",
      "   - Efficace en haute dimension\n",
      "   - Tres sensible aux echelles (necessite standardisation)\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - X_train_scaled / X_test_scaled (toutes variables standardisees + encodees)\n",
      "   - Raison : SVM est tres sensible aux echelles\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ENTRAINEMENT DU MODELE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Entrainement sur 800 echantillons avec 21 features...\n",
      "Entrainement termine\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREDICTIONS ET EVALUATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE D'ENTRAINEMENT :\n",
      "   Precision globale   : 96.50%\n",
      "   Precision positive  : 88.39%\n",
      "   Recall              : 99.00%\n",
      "   F1-Score            : 93.40%\n",
      "   ROC-AUC             : 99.89%\n",
      "\n",
      "PERFORMANCES SUR L'ENSEMBLE DE TEST :\n",
      "   Precision globale   : 91.50%\n",
      "   Precision positive  : 77.05%\n",
      "   Recall              : 94.00%\n",
      "   F1-Score            : 84.68%\n",
      "   ROC-AUC             : 97.53%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MATRICE DE CONFUSION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " [[136  14]\n",
      " [  3  47]]\n",
      "\n",
      "   Vrais Negatifs  (TN) : 136\n",
      "   Faux Positifs   (FP) : 14\n",
      "   Faux Negatifs   (FN) : 3\n",
      "   Vrais Positifs  (TP) : 47\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RAPPORT DE CLASSIFICATION (TEST)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Pas infarctus       0.98      0.91      0.94       150\n",
      "    Infarctus       0.77      0.94      0.85        50\n",
      "\n",
      "     accuracy                           0.92       200\n",
      "    macro avg       0.87      0.92      0.89       200\n",
      " weighted avg       0.93      0.92      0.92       200\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Calcul de l'importance par permutation (peut prendre quelques secondes)...\n",
      "\n",
      "               feature  importance\n",
      "   douleur_thoracique      0.0520\n",
      "              dyspnee      0.0440\n",
      "                  ldl      0.0420\n",
      "             glycemie      0.0395\n",
      "                  age      0.0345\n",
      "                 sexe      0.0315\n",
      "              diabete      0.0305\n",
      " hypercholesterolemie      0.0265\n",
      "  pression_systolique      0.0260\n",
      "         hypertension      0.0190\n",
      "    cholesterol_total      0.0185\n",
      "          tabagisme_2      0.0140\n",
      " pression_diastolique      0.0060\n",
      "  frequence_cardiaque      0.0050\n",
      "                  imc      0.0020\n",
      "                  hdl      0.0015\n",
      "        triglycerides     -0.0010\n",
      "antecedents_familiaux     -0.0010\n",
      "                  crp     -0.0015\n",
      "          tabagisme_1     -0.0015\n",
      "\n",
      "================================================================================\n",
      "SVM TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:36.735329900Z",
     "start_time": "2026-03-01T05:18:36.512943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "13. MODELE NON SUPERVISE : DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"13. MODELE NON SUPERVISE : DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Distance de Gower : adaptee aux donnees mixtes (numeriques + categorielles + binaires)\n",
    "   - Clustering hierarchique : structure hierarchique des patients\n",
    "   - Methode la plus robuste pour donnees heterogenes\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - df_final_sans_cible (toutes variables, format original apres imputation)\n",
    "   - Raison : Gower gere nativement tous les types de variables\n",
    "   - PAS de standardisation ni d'encodage necessaire\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CALCUL DE LA MATRICE DE DISTANCE DE GOWER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nCalcul de la matrice de distance sur {df_final_sans_cible.shape[0]} patients...\")\n",
    "print(\"Cette operation peut prendre quelques secondes...\")\n",
    "\n",
    "# Calculer la matrice de distance de Gower\n",
    "gower_matrix = gower.gower_matrix(df_final_sans_cible)\n",
    "\n",
    "print(f\"Matrice de Gower calculee : {gower_matrix.shape[0]} x {gower_matrix.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"CLUSTERING HIERARCHIQUE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Convertir en format condensed pour linkage\n",
    "gower_dist_condensed = squareform(gower_matrix, checks=False)\n",
    "\n",
    "# Clustering hierarchique avec methode Ward\n",
    "print(\"\\nMethode : Ward (minimise la variance intra-cluster)\")\n",
    "linkage_matrix = linkage(gower_dist_condensed, method='ward')\n",
    "\n",
    "print(\"Linkage matrix calculee\")\n",
    "\n",
    "# Tester differents nombres de clusters\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultats_gower_clusters = {}\n",
    "\n",
    "for n_clusters in [2, 3, 4, 5]:\n",
    "    # Couper le dendrogramme pour obtenir n clusters\n",
    "    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "\n",
    "    # Calculer les metriques\n",
    "    silhouette = silhouette_score(gower_matrix, cluster_labels, metric='precomputed')\n",
    "    davies_bouldin = davies_bouldin_score(gower_matrix, cluster_labels)\n",
    "\n",
    "    # Analyser la distribution des infarctus dans chaque cluster\n",
    "    df_temp = df_final.copy()\n",
    "    df_temp['cluster'] = cluster_labels\n",
    "\n",
    "    print(f\"\\nNOMBRE DE CLUSTERS : {n_clusters}\")\n",
    "    print(f\"   Silhouette Score      : {silhouette*100:.2f}% (plus proche de 100% = mieux)\")\n",
    "    print(f\"   Davies-Bouldin Score  : {davies_bouldin:.2f} (plus proche de 0 = mieux)\")\n",
    "\n",
    "    print(f\"\\n   Distribution des patients par cluster :\")\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        n_patients = (cluster_labels == cluster_id).sum()\n",
    "        pct_patients = (n_patients / len(cluster_labels)) * 100\n",
    "\n",
    "        # Taux d'infarctus dans ce cluster\n",
    "        cluster_mask = df_temp['cluster'] == cluster_id\n",
    "        n_infarctus = df_temp.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux_infarctus = (n_infarctus / n_patients) * 100 if n_patients > 0 else 0\n",
    "\n",
    "        print(f\"      Cluster {cluster_id} : {n_patients:4d} patients ({pct_patients:5.1f}%) - Taux infarctus : {taux_infarctus:5.1f}%\")\n",
    "\n",
    "    resultats_gower_clusters[n_clusters] = {\n",
    "        'labels': cluster_labels,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "\n",
    "# Selectionner le meilleur nombre de clusters (basé sur Silhouette Score)\n",
    "best_n_clusters_gower = max(resultats_gower_clusters.keys(),\n",
    "                             key=lambda k: resultats_gower_clusters[k]['silhouette'])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"MEILLEUR NOMBRE DE CLUSTERS : {best_n_clusters_gower}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Basé sur le Silhouette Score le plus eleve\")\n",
    "print(f\"   Silhouette Score : {resultats_gower_clusters[best_n_clusters_gower]['silhouette']*100:.2f}%\")\n",
    "\n",
    "# Utiliser le meilleur clustering\n",
    "best_cluster_labels_gower = resultats_gower_clusters[best_n_clusters_gower]['labels']\n",
    "\n",
    "# Ajouter les clusters au dataframe pour analyse ulterieure\n",
    "df_final['cluster_gower'] = best_cluster_labels_gower\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pour evaluer l'importance des variables dans le clustering,\n",
    "on calcule la contribution de chaque variable a la separation des clusters.\n",
    "Methode : Variance inter-cluster / Variance totale\n",
    "\"\"\")\n",
    "\n",
    "# Calculer l'importance des variables basee sur la variance inter-cluster\n",
    "df_avec_clusters = df_final_sans_cible.copy()\n",
    "df_avec_clusters['cluster'] = best_cluster_labels_gower\n",
    "\n",
    "importance_gower = []\n",
    "\n",
    "for col in df_final_sans_cible.columns:\n",
    "    # Variance totale\n",
    "    if df_final_sans_cible[col].dtype in ['int64', 'float64']:\n",
    "        variance_totale = df_final_sans_cible[col].var()\n",
    "\n",
    "        # Variance inter-cluster (entre les moyennes des clusters)\n",
    "        moyennes_clusters = df_avec_clusters.groupby('cluster')[col].mean()\n",
    "        variance_inter = moyennes_clusters.var()\n",
    "\n",
    "        # Ratio variance inter / variance totale\n",
    "        if variance_totale > 0:\n",
    "            ratio = variance_inter / variance_totale\n",
    "        else:\n",
    "            ratio = 0.0\n",
    "    else:\n",
    "        # Pour les variables categorielles, on utilise l'entropie\n",
    "        ratio = 0.0\n",
    "\n",
    "    importance_gower.append({'feature': col, 'importance': ratio})\n",
    "\n",
    "feature_importance_gower = pd.DataFrame(importance_gower).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "for idx, row in feature_importance_gower.head(20).iterrows():\n",
    "    print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['Gower + Hierarchical'] = {\n",
    "    'gower_matrix': gower_matrix,\n",
    "    'linkage_matrix': linkage_matrix,\n",
    "    'best_n_clusters': best_n_clusters_gower,\n",
    "    'cluster_labels': best_cluster_labels_gower,\n",
    "    'resultats_par_k': resultats_gower_clusters,\n",
    "    'feature_importance': feature_importance_gower\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "5affee76e1c1d8d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "13. MODELE NON SUPERVISE : DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Distance de Gower : adaptee aux donnees mixtes (numeriques + categorielles + binaires)\n",
      "   - Clustering hierarchique : structure hierarchique des patients\n",
      "   - Methode la plus robuste pour donnees heterogenes\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - df_final_sans_cible (toutes variables, format original apres imputation)\n",
      "   - Raison : Gower gere nativement tous les types de variables\n",
      "   - PAS de standardisation ni d'encodage necessaire\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CALCUL DE LA MATRICE DE DISTANCE DE GOWER\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Calcul de la matrice de distance sur 1000 patients...\n",
      "Cette operation peut prendre quelques secondes...\n",
      "Matrice de Gower calculee : 1000 x 1000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTERING HIERARCHIQUE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Methode : Ward (minimise la variance intra-cluster)\n",
      "Linkage matrix calculee\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NOMBRE DE CLUSTERS : 2\n",
      "   Silhouette Score      : 14.11% (plus proche de 100% = mieux)\n",
      "   Davies-Bouldin Score  : 2.51 (plus proche de 0 = mieux)\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 1 :  337 patients ( 33.7%) - Taux infarctus :   3.9%\n",
      "      Cluster 2 :  663 patients ( 66.3%) - Taux infarctus :  35.7%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 3\n",
      "   Silhouette Score      : 13.61% (plus proche de 100% = mieux)\n",
      "   Davies-Bouldin Score  : 2.26 (plus proche de 0 = mieux)\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 1 :  337 patients ( 33.7%) - Taux infarctus :   3.9%\n",
      "      Cluster 2 :  286 patients ( 28.6%) - Taux infarctus :  63.3%\n",
      "      Cluster 3 :  377 patients ( 37.7%) - Taux infarctus :  14.9%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 4\n",
      "   Silhouette Score      : 10.80% (plus proche de 100% = mieux)\n",
      "   Davies-Bouldin Score  : 2.18 (plus proche de 0 = mieux)\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 1 :  337 patients ( 33.7%) - Taux infarctus :   3.9%\n",
      "      Cluster 2 :  286 patients ( 28.6%) - Taux infarctus :  63.3%\n",
      "      Cluster 3 :  194 patients ( 19.4%) - Taux infarctus :  18.6%\n",
      "      Cluster 4 :  183 patients ( 18.3%) - Taux infarctus :  10.9%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 5\n",
      "   Silhouette Score      : 12.54% (plus proche de 100% = mieux)\n",
      "   Davies-Bouldin Score  : 2.23 (plus proche de 0 = mieux)\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 1 :  146 patients ( 14.6%) - Taux infarctus :   8.2%\n",
      "      Cluster 2 :  191 patients ( 19.1%) - Taux infarctus :   0.5%\n",
      "      Cluster 3 :  286 patients ( 28.6%) - Taux infarctus :  63.3%\n",
      "      Cluster 4 :  194 patients ( 19.4%) - Taux infarctus :  18.6%\n",
      "      Cluster 5 :  183 patients ( 18.3%) - Taux infarctus :  10.9%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MEILLEUR NOMBRE DE CLUSTERS : 2\n",
      "--------------------------------------------------------------------------------\n",
      "   Basé sur le Silhouette Score le plus eleve\n",
      "   Silhouette Score : 14.11%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pour evaluer l'importance des variables dans le clustering,\n",
      "on calcule la contribution de chaque variable a la separation des clusters.\n",
      "Methode : Variance inter-cluster / Variance totale\n",
      "\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   hypertension                   : 1.741505\n",
      "   pression_systolique            : 0.604953\n",
      "   pression_diastolique           : 0.465238\n",
      "   age                            : 0.286060\n",
      "   douleur_thoracique             : 0.219589\n",
      "   crp                            : 0.054821\n",
      "   hypercholesterolemie           : 0.017447\n",
      "   sexe                           : 0.010770\n",
      "   sedentarite                    : 0.007319\n",
      "   dyspnee                        : 0.006163\n",
      "   tabagisme                      : 0.003987\n",
      "   hdl                            : 0.001749\n",
      "   glycemie                       : 0.001510\n",
      "   imc                            : 0.001433\n",
      "   diabete                        : 0.001313\n",
      "   cholesterol_total              : 0.001291\n",
      "   ldl                            : 0.000198\n",
      "   frequence_cardiaque            : 0.000040\n",
      "   triglycerides                  : 0.000010\n",
      "   antecedents_familiaux          : 0.000006\n",
      "\n",
      "================================================================================\n",
      "DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:36.770811400Z",
     "start_time": "2026-03-01T05:18:36.736328300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "14. MODELE NON SUPERVISE : PCA (ANALYSE EN COMPOSANTES PRINCIPALES)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"14. MODELE NON SUPERVISE : PCA (ANALYSE EN COMPOSANTES PRINCIPALES)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Reduction de dimensionnalite\n",
    "   - Trouve les directions de variance maximale\n",
    "   - Permet la visualisation en 2D/3D\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - df_final_sans_cible avec uniquement variables numeriques standardisees\n",
    "   - Raison : PCA necessite des variables numeriques standardisees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREPARATION DES DONNEES POUR PCA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extraire uniquement les variables numeriques du dataframe complet\n",
    "df_num_for_pca = df_final[variables_numeriques].copy()\n",
    "\n",
    "# Standardiser\n",
    "scaler_pca = StandardScaler()\n",
    "df_num_scaled_pca = scaler_pca.fit_transform(df_num_for_pca)\n",
    "\n",
    "print(f\"\\nDonnees pour PCA : {df_num_scaled_pca.shape[0]} lignes x {df_num_scaled_pca.shape[1]} variables numeriques\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"APPLICATION DE LA PCA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Appliquer PCA avec toutes les composantes\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(df_num_scaled_pca)\n",
    "\n",
    "# Variance expliquee cumulee\n",
    "variance_cumulee = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(\"\\nVariance expliquee par les composantes principales :\")\n",
    "for i in range(min(10, len(pca_full.explained_variance_ratio_))):\n",
    "    print(f\"   PC{i+1:2d} : {pca_full.explained_variance_ratio_[i]:.4f} ({variance_cumulee[i]:.4f} cumulee)\")\n",
    "\n",
    "# Trouver le nombre de composantes pour expliquer 80% et 95% de la variance\n",
    "n_comp_80 = np.argmax(variance_cumulee >= 0.80) + 1\n",
    "n_comp_95 = np.argmax(variance_cumulee >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nNombre de composantes pour expliquer :\")\n",
    "print(f\"   80% de la variance : {n_comp_80} composantes\")\n",
    "print(f\"   95% de la variance : {n_comp_95} composantes\")\n",
    "\n",
    "# Appliquer PCA avec reduction a 2 composantes pour visualisation\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PCA AVEC 2 COMPOSANTES (POUR VISUALISATION)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "principal_components_2d = pca_2d.fit_transform(df_num_scaled_pca)\n",
    "\n",
    "variance_expliquee_2d = pca_2d.explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"\\nVariance expliquee par PC1 et PC2 : {variance_expliquee_2d:.4f} ({variance_expliquee_2d*100:.2f}%)\")\n",
    "print(f\"   PC1 : {pca_2d.explained_variance_ratio_[0]:.4f} ({pca_2d.explained_variance_ratio_[0]*100:.2f}%)\")\n",
    "print(f\"   PC2 : {pca_2d.explained_variance_ratio_[1]:.4f} ({pca_2d.explained_variance_ratio_[1]*100:.2f}%)\")\n",
    "\n",
    "# Creer un dataframe avec les composantes principales\n",
    "df_pca_2d = pd.DataFrame(\n",
    "    principal_components_2d,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=df_final.index\n",
    ")\n",
    "\n",
    "# Ajouter la variable cible pour analyse\n",
    "df_pca_2d['infarctus'] = df_final['infarctus'].values\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANALYSE DES COMPOSANTES PRINCIPALES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Top variables contribuant a PC1 et PC2\n",
    "loadings = pd.DataFrame(\n",
    "    pca_2d.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=variables_numeriques\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 variables contribuant a PC1 :\")\n",
    "top_pc1 = loadings['PC1'].abs().sort_values(ascending=False).head()\n",
    "for var, val in top_pc1.items():\n",
    "    print(f\"   {var:30s} : {loadings.loc[var, 'PC1']:7.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 variables contribuant a PC2 :\")\n",
    "top_pc2 = loadings['PC2'].abs().sort_values(ascending=False).head()\n",
    "for var, val in top_pc2.items():\n",
    "    print(f\"   {var:30s} : {loadings.loc[var, 'PC2']:7.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pour la PCA, l'importance d'une variable est mesuree par sa contribution globale\n",
    "aux composantes principales (moyenne des valeurs absolues des loadings).\n",
    "\"\"\")\n",
    "\n",
    "# Calculer l'importance globale de chaque variable\n",
    "# Moyenne des valeurs absolues des loadings sur toutes les composantes\n",
    "importance_pca = pd.DataFrame({\n",
    "    'feature': variables_numeriques,\n",
    "    'importance': np.abs(pca_full.components_).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "for idx, row in importance_pca.head(20).iterrows():\n",
    "    print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['PCA'] = {\n",
    "    'pca_full': pca_full,\n",
    "    'pca_2d': pca_2d,\n",
    "    'principal_components_2d': principal_components_2d,\n",
    "    'variance_expliquee_2d': variance_expliquee_2d,\n",
    "    'n_comp_80': n_comp_80,\n",
    "    'n_comp_95': n_comp_95,\n",
    "    'loadings': loadings,\n",
    "    'feature_importance': importance_pca\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PCA TERMINEE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "916471073cafc1c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "14. MODELE NON SUPERVISE : PCA (ANALYSE EN COMPOSANTES PRINCIPALES)\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Reduction de dimensionnalite\n",
      "   - Trouve les directions de variance maximale\n",
      "   - Permet la visualisation en 2D/3D\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - df_final_sans_cible avec uniquement variables numeriques standardisees\n",
      "   - Raison : PCA necessite des variables numeriques standardisees\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREPARATION DES DONNEES POUR PCA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Donnees pour PCA : 1000 lignes x 11 variables numeriques\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "APPLICATION DE LA PCA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variance expliquee par les composantes principales :\n",
      "   PC 1 : 0.1681 (0.1681 cumulee)\n",
      "   PC 2 : 0.1488 (0.3169 cumulee)\n",
      "   PC 3 : 0.1322 (0.4491 cumulee)\n",
      "   PC 4 : 0.1009 (0.5501 cumulee)\n",
      "   PC 5 : 0.0856 (0.6356 cumulee)\n",
      "   PC 6 : 0.0813 (0.7169 cumulee)\n",
      "   PC 7 : 0.0781 (0.7950 cumulee)\n",
      "   PC 8 : 0.0707 (0.8657 cumulee)\n",
      "   PC 9 : 0.0666 (0.9322 cumulee)\n",
      "   PC10 : 0.0519 (0.9841 cumulee)\n",
      "\n",
      "Nombre de composantes pour expliquer :\n",
      "   80% de la variance : 8 composantes\n",
      "   95% de la variance : 10 composantes\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PCA AVEC 2 COMPOSANTES (POUR VISUALISATION)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variance expliquee par PC1 et PC2 : 0.3169 (31.69%)\n",
      "   PC1 : 0.1681 (16.81%)\n",
      "   PC2 : 0.1488 (14.88%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANALYSE DES COMPOSANTES PRINCIPALES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 variables contribuant a PC1 :\n",
      "   ldl                            :  0.6887\n",
      "   cholesterol_total              :  0.6862\n",
      "   imc                            :  0.1359\n",
      "   glycemie                       :  0.1278\n",
      "   hdl                            : -0.1007\n",
      "\n",
      "Top 5 variables contribuant a PC2 :\n",
      "   glycemie                       :  0.5636\n",
      "   imc                            :  0.4489\n",
      "   triglycerides                  :  0.4377\n",
      "   crp                            :  0.3241\n",
      "   age                            :  0.2257\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pour la PCA, l'importance d'une variable est mesuree par sa contribution globale\n",
      "aux composantes principales (moyenne des valeurs absolues des loadings).\n",
      "\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   triglycerides                  : 0.254051\n",
      "   imc                            : 0.244385\n",
      "   age                            : 0.225238\n",
      "   pression_diastolique           : 0.220251\n",
      "   pression_systolique            : 0.210346\n",
      "   frequence_cardiaque            : 0.209258\n",
      "   glycemie                       : 0.201013\n",
      "   crp                            : 0.200635\n",
      "   hdl                            : 0.191633\n",
      "   cholesterol_total              : 0.162841\n",
      "   ldl                            : 0.160417\n",
      "\n",
      "================================================================================\n",
      "PCA TERMINEE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:19:08.321191300Z",
     "start_time": "2026-03-01T05:19:07.143002100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "15. MODELE NON SUPERVISE : K-MEANS (VERSION NUMERIQUE)\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Clustering par partitionnement\n",
      "   - Utilise la distance euclidienne\n",
      "   - Necessite de specifier le nombre de clusters\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - Variables numeriques standardisees UNIQUEMENT\n",
      "   - Raison : Eviter le biais lie aux variables binaires encodees\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREPARATION DES DONNEES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Donnees pour K-Means : 1000 lignes x 11 variables numeriques\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NOMBRE DE CLUSTERS : 2\n",
      "   Inertia               : 9794.22\n",
      "   Silhouette Score      : 10.19%\n",
      "   Davies-Bouldin Score  : 2.79\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  506 patients ( 50.6%) - Taux infarctus :  14.2%\n",
      "      Cluster 1 :  494 patients ( 49.4%) - Taux infarctus :  36.0%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 3\n",
      "   Inertia               : 9039.42\n",
      "   Silhouette Score      : 9.25%\n",
      "   Davies-Bouldin Score  : 2.45\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  361 patients ( 36.1%) - Taux infarctus :   8.0%\n",
      "      Cluster 1 :  325 patients ( 32.5%) - Taux infarctus :  47.7%\n",
      "      Cluster 2 :  314 patients ( 31.4%) - Taux infarctus :  21.0%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 4\n",
      "   Inertia               : 8510.93\n",
      "   Silhouette Score      : 9.16%\n",
      "   Davies-Bouldin Score  : 2.33\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  302 patients ( 30.2%) - Taux infarctus :   7.3%\n",
      "      Cluster 1 :  241 patients ( 24.1%) - Taux infarctus :  10.0%\n",
      "      Cluster 2 :  252 patients ( 25.2%) - Taux infarctus :  29.4%\n",
      "      Cluster 3 :  205 patients ( 20.5%) - Taux infarctus :  63.4%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 5\n",
      "   Inertia               : 8075.69\n",
      "   Silhouette Score      : 9.24%\n",
      "   Davies-Bouldin Score  : 2.21\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  219 patients ( 21.9%) - Taux infarctus :   0.9%\n",
      "      Cluster 1 :  231 patients ( 23.1%) - Taux infarctus :  12.1%\n",
      "      Cluster 2 :  217 patients ( 21.7%) - Taux infarctus :  30.9%\n",
      "      Cluster 3 :  204 patients ( 20.4%) - Taux infarctus :  39.2%\n",
      "      Cluster 4 :  129 patients ( 12.9%) - Taux infarctus :  56.6%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 6\n",
      "   Inertia               : 7777.24\n",
      "   Silhouette Score      : 8.93%\n",
      "   Davies-Bouldin Score  : 2.23\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  218 patients ( 21.8%) - Taux infarctus :  10.1%\n",
      "      Cluster 1 :  174 patients ( 17.4%) - Taux infarctus :   1.7%\n",
      "      Cluster 2 :  117 patients ( 11.7%) - Taux infarctus :  53.0%\n",
      "      Cluster 3 :  167 patients ( 16.7%) - Taux infarctus :  26.9%\n",
      "      Cluster 4 :  159 patients ( 15.9%) - Taux infarctus :  57.2%\n",
      "      Cluster 5 :  165 patients ( 16.5%) - Taux infarctus :  16.4%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 7\n",
      "   Inertia               : 7518.13\n",
      "   Silhouette Score      : 8.73%\n",
      "   Davies-Bouldin Score  : 2.18\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  143 patients ( 14.3%) - Taux infarctus :   9.1%\n",
      "      Cluster 1 :  114 patients ( 11.4%) - Taux infarctus :  53.5%\n",
      "      Cluster 2 :  158 patients ( 15.8%) - Taux infarctus :  17.1%\n",
      "      Cluster 3 :  158 patients ( 15.8%) - Taux infarctus :   1.3%\n",
      "      Cluster 4 :  148 patients ( 14.8%) - Taux infarctus :  25.0%\n",
      "      Cluster 5 :  141 patients ( 14.1%) - Taux infarctus :  13.5%\n",
      "      Cluster 6 :  138 patients ( 13.8%) - Taux infarctus :  65.9%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 8\n",
      "   Inertia               : 7300.19\n",
      "   Silhouette Score      : 8.50%\n",
      "   Davies-Bouldin Score  : 2.15\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  143 patients ( 14.3%) - Taux infarctus :   4.2%\n",
      "      Cluster 1 :  132 patients ( 13.2%) - Taux infarctus :   9.1%\n",
      "      Cluster 2 :  117 patients ( 11.7%) - Taux infarctus :   5.1%\n",
      "      Cluster 3 :  126 patients ( 12.6%) - Taux infarctus :  66.7%\n",
      "      Cluster 4 :  106 patients ( 10.6%) - Taux infarctus :  54.7%\n",
      "      Cluster 5 :  117 patients ( 11.7%) - Taux infarctus :  11.1%\n",
      "      Cluster 6 :  130 patients ( 13.0%) - Taux infarctus :  20.8%\n",
      "      Cluster 7 :  129 patients ( 12.9%) - Taux infarctus :  34.1%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 9\n",
      "   Inertia               : 7136.05\n",
      "   Silhouette Score      : 8.45%\n",
      "   Davies-Bouldin Score  : 2.16\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  135 patients ( 13.5%) - Taux infarctus :   1.5%\n",
      "      Cluster 1 :   95 patients (  9.5%) - Taux infarctus :  16.8%\n",
      "      Cluster 2 :  116 patients ( 11.6%) - Taux infarctus :  31.0%\n",
      "      Cluster 3 :   78 patients (  7.8%) - Taux infarctus :  46.2%\n",
      "      Cluster 4 :  110 patients ( 11.0%) - Taux infarctus :  54.5%\n",
      "      Cluster 5 :  129 patients ( 12.9%) - Taux infarctus :  13.2%\n",
      "      Cluster 6 :  125 patients ( 12.5%) - Taux infarctus :  17.6%\n",
      "      Cluster 7 :   93 patients (  9.3%) - Taux infarctus :  60.2%\n",
      "      Cluster 8 :  119 patients ( 11.9%) - Taux infarctus :   4.2%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 10\n",
      "   Inertia               : 6973.15\n",
      "   Silhouette Score      : 8.47%\n",
      "   Davies-Bouldin Score  : 2.12\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  104 patients ( 10.4%) - Taux infarctus :   2.9%\n",
      "      Cluster 1 :  104 patients ( 10.4%) - Taux infarctus :  52.9%\n",
      "      Cluster 2 :   90 patients (  9.0%) - Taux infarctus :  42.2%\n",
      "      Cluster 3 :  101 patients ( 10.1%) - Taux infarctus :  16.8%\n",
      "      Cluster 4 :  126 patients ( 12.6%) - Taux infarctus :  18.3%\n",
      "      Cluster 5 :  119 patients ( 11.9%) - Taux infarctus :  16.0%\n",
      "      Cluster 6 :  103 patients ( 10.3%) - Taux infarctus :   2.9%\n",
      "      Cluster 7 :   97 patients (  9.7%) - Taux infarctus :  28.9%\n",
      "      Cluster 8 :   77 patients (  7.7%) - Taux infarctus :  71.4%\n",
      "      Cluster 9 :   79 patients (  7.9%) - Taux infarctus :  11.4%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 11\n",
      "   Inertia               : 6821.13\n",
      "   Silhouette Score      : 8.37%\n",
      "   Davies-Bouldin Score  : 2.10\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  109 patients ( 10.9%) - Taux infarctus :  12.8%\n",
      "      Cluster 1 :  121 patients ( 12.1%) - Taux infarctus :  11.6%\n",
      "      Cluster 2 :   75 patients (  7.5%) - Taux infarctus :  21.3%\n",
      "      Cluster 3 :   80 patients (  8.0%) - Taux infarctus :   3.8%\n",
      "      Cluster 4 :  100 patients ( 10.0%) - Taux infarctus :  57.0%\n",
      "      Cluster 5 :  102 patients ( 10.2%) - Taux infarctus :  15.7%\n",
      "      Cluster 6 :   93 patients (  9.3%) - Taux infarctus :   1.1%\n",
      "      Cluster 7 :   74 patients (  7.4%) - Taux infarctus :  50.0%\n",
      "      Cluster 8 :  100 patients ( 10.0%) - Taux infarctus :  35.0%\n",
      "      Cluster 9 :   80 patients (  8.0%) - Taux infarctus :  63.7%\n",
      "      Cluster 10 :   66 patients (  6.6%) - Taux infarctus :   9.1%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MEILLEUR NOMBRE DE CLUSTERS : 2\n",
      "--------------------------------------------------------------------------------\n",
      "   Basé sur le Silhouette Score le plus eleve\n",
      "   Silhouette Score : 10.19%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pour K-Means, l'importance d'une variable est mesuree par sa contribution\n",
      "a la separation des clusters (variance inter-cluster / variance totale).\n",
      "\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   ldl                            : 1.152755\n",
      "   cholesterol_total              : 1.110600\n",
      "   hdl                            : 0.053037\n",
      "   imc                            : 0.044445\n",
      "   frequence_cardiaque            : 0.018646\n",
      "   glycemie                       : 0.008261\n",
      "   pression_diastolique           : 0.007933\n",
      "   crp                            : 0.005874\n",
      "   pression_systolique            : 0.005356\n",
      "   triglycerides                  : 0.002344\n",
      "   age                            : 0.000239\n",
      "\n",
      "================================================================================\n",
      "K-MEANS (VERSION NUMERIQUE) TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 21,
   "source": [
    "\"\"\"\n",
    "15. MODELE NON SUPERVISE : K-MEANS (VERSION NUMERIQUE)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"15. MODELE NON SUPERVISE : K-MEANS (VERSION NUMERIQUE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Clustering par partitionnement\n",
    "   - Utilise la distance euclidienne\n",
    "   - Necessite de specifier le nombre de clusters\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - Variables numeriques standardisees UNIQUEMENT\n",
    "   - Raison : Eviter le biais lie aux variables binaires encodees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREPARATION DES DONNEES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Utiliser les memes donnees que pour PCA (variables numeriques standardisees)\n",
    "X_kmeans_num = df_num_scaled_pca.copy()\n",
    "\n",
    "print(f\"\\nDonnees pour K-Means : {X_kmeans_num.shape[0]} lignes x {X_kmeans_num.shape[1]} variables numeriques\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultats_kmeans_num = {}\n",
    "inertias_num = []\n",
    "\n",
    "for n_clusters in range(2, 8):\n",
    "    # Appliquer K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_kmeans_num)\n",
    "\n",
    "    # Metriques\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X_kmeans_num, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_kmeans_num, cluster_labels)\n",
    "\n",
    "    inertias_num.append(inertia)\n",
    "\n",
    "    # Analyser la distribution des infarctus dans chaque cluster\n",
    "    df_temp = df_final.copy()\n",
    "    df_temp['cluster'] = cluster_labels\n",
    "\n",
    "    print(f\"\\nNOMBRE DE CLUSTERS : {n_clusters}\")\n",
    "    print(f\"   Inertia               : {inertia:.2f}\")\n",
    "    print(f\"   Silhouette Score      : {silhouette*100:.2f}%\")\n",
    "    print(f\"   Davies-Bouldin Score  : {davies_bouldin:.2f}\")\n",
    "\n",
    "    print(f\"\\n   Distribution des patients par cluster :\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        n_patients = (cluster_labels == cluster_id).sum()\n",
    "        pct_patients = (n_patients / len(cluster_labels)) * 100\n",
    "\n",
    "        # Taux d'infarctus dans ce cluster\n",
    "        cluster_mask = df_temp['cluster'] == cluster_id\n",
    "        n_infarctus = df_temp.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux_infarctus = (n_infarctus / n_patients) * 100 if n_patients > 0 else 0\n",
    "\n",
    "        print(f\"      Cluster {cluster_id} : {n_patients:4d} patients ({pct_patients:5.1f}%) - Taux infarctus : {taux_infarctus:5.1f}%\")\n",
    "\n",
    "    resultats_kmeans_num[n_clusters] = {\n",
    "        'model': kmeans,\n",
    "        'labels': cluster_labels,\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "\n",
    "# Selectionner le meilleur nombre de clusters (basé sur Silhouette Score)\n",
    "best_n_clusters_kmeans_num = max(resultats_kmeans_num.keys(),\n",
    "                                  key=lambda k: resultats_kmeans_num[k]['silhouette'])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"MEILLEUR NOMBRE DE CLUSTERS : {best_n_clusters_kmeans_num}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Basé sur le Silhouette Score le plus eleve\")\n",
    "print(f\"   Silhouette Score : {resultats_kmeans_num[best_n_clusters_kmeans_num]['silhouette']*100:.2f}%\")\n",
    "\n",
    "# Utiliser le meilleur clustering\n",
    "best_cluster_labels_kmeans_num = resultats_kmeans_num[best_n_clusters_kmeans_num]['labels']\n",
    "\n",
    "# Ajouter les clusters au dataframe\n",
    "df_final['cluster_kmeans_num'] = best_cluster_labels_kmeans_num\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pour K-Means, l'importance d'une variable est mesuree par sa contribution\n",
    "a la separation des clusters (variance inter-cluster / variance totale).\n",
    "\"\"\")\n",
    "\n",
    "# Calculer l'importance des variables\n",
    "df_kmeans_num_with_clusters = pd.DataFrame(\n",
    "    df_num_scaled_pca,\n",
    "    columns=variables_numeriques,\n",
    "    index=df_final.index\n",
    ")\n",
    "df_kmeans_num_with_clusters['cluster'] = best_cluster_labels_kmeans_num\n",
    "\n",
    "importance_kmeans_num = []\n",
    "\n",
    "for col in variables_numeriques:\n",
    "    # Variance totale\n",
    "    variance_totale = df_kmeans_num_with_clusters[col].var()\n",
    "\n",
    "    # Variance inter-cluster\n",
    "    moyennes_clusters = df_kmeans_num_with_clusters.groupby('cluster')[col].mean()\n",
    "    variance_inter = moyennes_clusters.var()\n",
    "\n",
    "    # Ratio\n",
    "    if variance_totale > 0:\n",
    "        ratio = variance_inter / variance_totale\n",
    "    else:\n",
    "        ratio = 0.0\n",
    "\n",
    "    importance_kmeans_num.append({'feature': col, 'importance': ratio})\n",
    "\n",
    "feature_importance_kmeans_num = pd.DataFrame(importance_kmeans_num).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "for idx, row in feature_importance_kmeans_num.head(20).iterrows():\n",
    "    print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['K-Means (numerique)'] = {\n",
    "    'best_model': resultats_kmeans_num[best_n_clusters_kmeans_num]['model'],\n",
    "    'best_n_clusters': best_n_clusters_kmeans_num,\n",
    "    'cluster_labels': best_cluster_labels_kmeans_num,\n",
    "    'resultats_par_k': resultats_kmeans_num,\n",
    "    'inertias': inertias_num,\n",
    "    'feature_importance': feature_importance_kmeans_num\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K-MEANS (VERSION NUMERIQUE) TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "a326d69f251d111f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:39.200124600Z",
     "start_time": "2026-03-01T05:18:38.231138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "16. MODELE NON SUPERVISE : K-MEANS (VERSION COMPLETE)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"16. MODELE NON SUPERVISE : K-MEANS (VERSION COMPLETE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "DESCRIPTION :\n",
    "   - Clustering par partitionnement\n",
    "   - Utilise la distance euclidienne\n",
    "   - Version avec TOUTES les variables (numeriques + binaires + categorielles encodees)\n",
    "\n",
    "DONNEES UTILISEES :\n",
    "   - Toutes variables (numeriques standardisees + binaires + categorielles encodees)\n",
    "   - Raison : Comparer l'impact de l'ajout des variables binaires/categorielles\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"PREPARATION DES DONNEES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Preparer les donnees completes pour le dataframe complet\n",
    "df_num_complete = df_final[variables_numeriques].copy()\n",
    "scaler_complete = StandardScaler()\n",
    "df_num_scaled_complete = scaler_complete.fit_transform(df_num_complete)\n",
    "df_num_scaled_complete = pd.DataFrame(df_num_scaled_complete, columns=variables_numeriques, index=df_final.index)\n",
    "\n",
    "# Ajouter les variables binaires\n",
    "df_bin_complete = df_final[variables_binaires].copy()\n",
    "\n",
    "# Encoder les variables categorielles si elles existent\n",
    "if len(variables_categorielles) > 0:\n",
    "    encoder_complete = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    df_cat_complete = df_final[variables_categorielles].copy()\n",
    "    cat_encoded = encoder_complete.fit_transform(df_cat_complete)\n",
    "\n",
    "    # Noms des colonnes encodees\n",
    "    encoded_names_complete = []\n",
    "    for i, var in enumerate(variables_categorielles):\n",
    "        categories = encoder_complete.categories_[i][1:]\n",
    "        for cat in categories:\n",
    "            encoded_names_complete.append(f\"{var}_{cat}\")\n",
    "\n",
    "    df_cat_encoded_complete = pd.DataFrame(cat_encoded, columns=encoded_names_complete, index=df_final.index)\n",
    "\n",
    "    # Combiner tout\n",
    "    X_kmeans_complete = pd.concat([df_num_scaled_complete, df_bin_complete, df_cat_encoded_complete], axis=1)\n",
    "else:\n",
    "    X_kmeans_complete = pd.concat([df_num_scaled_complete, df_bin_complete], axis=1)\n",
    "\n",
    "print(f\"\\nDonnees pour K-Means : {X_kmeans_complete.shape[0]} lignes x {X_kmeans_complete.shape[1]} features\")\n",
    "print(f\"   - Variables numeriques standardisees : {len(variables_numeriques)}\")\n",
    "print(f\"   - Variables binaires                 : {len(variables_binaires)}\")\n",
    "if len(variables_categorielles) > 0:\n",
    "    print(f\"   - Variables categorielles encodees   : {len(encoded_names_complete)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "resultats_kmeans_complete = {}\n",
    "inertias_complete = []\n",
    "\n",
    "for n_clusters in range(2, 8):\n",
    "    # Appliquer K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_kmeans_complete)\n",
    "\n",
    "    # Metriques\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X_kmeans_complete, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_kmeans_complete, cluster_labels)\n",
    "\n",
    "    inertias_complete.append(inertia)\n",
    "\n",
    "    # Analyser la distribution des infarctus dans chaque cluster\n",
    "    df_temp = df_final.copy()\n",
    "    df_temp['cluster'] = cluster_labels\n",
    "\n",
    "    print(f\"\\nNOMBRE DE CLUSTERS : {n_clusters}\")\n",
    "    print(f\"   Inertia               : {inertia:.2f}\")\n",
    "    print(f\"   Silhouette Score      : {silhouette*100:.2f}%\")\n",
    "    print(f\"   Davies-Bouldin Score  : {davies_bouldin:.2f}\")\n",
    "\n",
    "    print(f\"\\n   Distribution des patients par cluster :\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        n_patients = (cluster_labels == cluster_id).sum()\n",
    "        pct_patients = (n_patients / len(cluster_labels)) * 100\n",
    "\n",
    "        # Taux d'infarctus dans ce cluster\n",
    "        cluster_mask = df_temp['cluster'] == cluster_id\n",
    "        n_infarctus = df_temp.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux_infarctus = (n_infarctus / n_patients) * 100 if n_patients > 0 else 0\n",
    "\n",
    "        print(f\"      Cluster {cluster_id} : {n_patients:4d} patients ({pct_patients:5.1f}%) - Taux infarctus : {taux_infarctus:5.1f}%\")\n",
    "\n",
    "    resultats_kmeans_complete[n_clusters] = {\n",
    "        'model': kmeans,\n",
    "        'labels': cluster_labels,\n",
    "        'inertia': inertia,\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin\n",
    "    }\n",
    "\n",
    "# Selectionner le meilleur nombre de clusters (basé sur Silhouette Score)\n",
    "best_n_clusters_kmeans_complete = max(resultats_kmeans_complete.keys(),\n",
    "                                       key=lambda k: resultats_kmeans_complete[k]['silhouette'])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"MEILLEUR NOMBRE DE CLUSTERS : {best_n_clusters_kmeans_complete}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Basé sur le Silhouette Score le plus eleve\")\n",
    "print(f\"   Silhouette Score : {resultats_kmeans_complete[best_n_clusters_kmeans_complete]['silhouette']*100:.2f}%\")\n",
    "\n",
    "# Utiliser le meilleur clustering\n",
    "best_cluster_labels_kmeans_complete = resultats_kmeans_complete[best_n_clusters_kmeans_complete]['labels']\n",
    "\n",
    "# Ajouter les clusters au dataframe\n",
    "df_final['cluster_kmeans_complete'] = best_cluster_labels_kmeans_complete\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Pour K-Means, l'importance d'une variable est mesuree par sa contribution\n",
    "a la separation des clusters (variance inter-cluster / variance totale).\n",
    "\"\"\")\n",
    "\n",
    "# Calculer l'importance des variables\n",
    "X_kmeans_complete_with_clusters = X_kmeans_complete.copy()\n",
    "X_kmeans_complete_with_clusters['cluster'] = best_cluster_labels_kmeans_complete\n",
    "\n",
    "importance_kmeans_complete = []\n",
    "\n",
    "for col in X_kmeans_complete.columns:\n",
    "    # Variance totale\n",
    "    variance_totale = X_kmeans_complete[col].var()\n",
    "\n",
    "    # Variance inter-cluster\n",
    "    moyennes_clusters = X_kmeans_complete_with_clusters.groupby('cluster')[col].mean()\n",
    "    variance_inter = moyennes_clusters.var()\n",
    "\n",
    "    # Ratio\n",
    "    if variance_totale > 0:\n",
    "        ratio = variance_inter / variance_totale\n",
    "    else:\n",
    "        ratio = 0.0\n",
    "\n",
    "    importance_kmeans_complete.append({'feature': col, 'importance': ratio})\n",
    "\n",
    "feature_importance_kmeans_complete = pd.DataFrame(importance_kmeans_complete).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "for idx, row in feature_importance_kmeans_complete.head(20).iterrows():\n",
    "    print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "# Stocker les resultats\n",
    "resultats_modeles['K-Means (complete)'] = {\n",
    "    'best_model': resultats_kmeans_complete[best_n_clusters_kmeans_complete]['model'],\n",
    "    'best_n_clusters': best_n_clusters_kmeans_complete,\n",
    "    'cluster_labels': best_cluster_labels_kmeans_complete,\n",
    "    'resultats_par_k': resultats_kmeans_complete,\n",
    "    'inertias': inertias_complete,\n",
    "    'feature_importance': feature_importance_kmeans_complete\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"K-MEANS (VERSION COMPLETE) TERMINE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "153d3f34ba863bca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "16. MODELE NON SUPERVISE : K-MEANS (VERSION COMPLETE)\n",
      "================================================================================\n",
      "\n",
      "DESCRIPTION :\n",
      "   - Clustering par partitionnement\n",
      "   - Utilise la distance euclidienne\n",
      "   - Version avec TOUTES les variables (numeriques + binaires + categorielles encodees)\n",
      "\n",
      "DONNEES UTILISEES :\n",
      "   - Toutes variables (numeriques standardisees + binaires + categorielles encodees)\n",
      "   - Raison : Comparer l'impact de l'ajout des variables binaires/categorielles\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PREPARATION DES DONNEES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Donnees pour K-Means : 1000 lignes x 21 features\n",
      "   - Variables numeriques standardisees : 11\n",
      "   - Variables binaires                 : 8\n",
      "   - Variables categorielles encodees   : 2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EVALUATION POUR DIFFERENTS NOMBRES DE CLUSTERS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "NOMBRE DE CLUSTERS : 2\n",
      "   Inertia               : 11866.21\n",
      "   Silhouette Score      : 9.91%\n",
      "   Davies-Bouldin Score  : 2.92\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  533 patients ( 53.3%) - Taux infarctus :   9.2%\n",
      "      Cluster 1 :  467 patients ( 46.7%) - Taux infarctus :  43.0%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 3\n",
      "   Inertia               : 11061.45\n",
      "   Silhouette Score      : 8.43%\n",
      "   Davies-Bouldin Score  : 2.63\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  330 patients ( 33.0%) - Taux infarctus :  30.3%\n",
      "      Cluster 1 :  371 patients ( 37.1%) - Taux infarctus :   5.7%\n",
      "      Cluster 2 :  299 patients ( 29.9%) - Taux infarctus :  43.1%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 4\n",
      "   Inertia               : 10458.67\n",
      "   Silhouette Score      : 7.92%\n",
      "   Davies-Bouldin Score  : 2.41\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  267 patients ( 26.7%) - Taux infarctus :   9.4%\n",
      "      Cluster 1 :  231 patients ( 23.1%) - Taux infarctus :   0.0%\n",
      "      Cluster 2 :  254 patients ( 25.4%) - Taux infarctus :  40.2%\n",
      "      Cluster 3 :  248 patients ( 24.8%) - Taux infarctus :  49.6%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 5\n",
      "   Inertia               : 10031.94\n",
      "   Silhouette Score      : 8.24%\n",
      "   Davies-Bouldin Score  : 2.39\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  204 patients ( 20.4%) - Taux infarctus :  43.6%\n",
      "      Cluster 1 :  213 patients ( 21.3%) - Taux infarctus :   0.0%\n",
      "      Cluster 2 :  130 patients ( 13.0%) - Taux infarctus :  56.9%\n",
      "      Cluster 3 :  238 patients ( 23.8%) - Taux infarctus :   9.7%\n",
      "      Cluster 4 :  215 patients ( 21.5%) - Taux infarctus :  29.8%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 6\n",
      "   Inertia               : 9704.46\n",
      "   Silhouette Score      : 7.57%\n",
      "   Davies-Bouldin Score  : 2.41\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  184 patients ( 18.4%) - Taux infarctus :  15.8%\n",
      "      Cluster 1 :  193 patients ( 19.3%) - Taux infarctus :   0.0%\n",
      "      Cluster 2 :  184 patients ( 18.4%) - Taux infarctus :   9.8%\n",
      "      Cluster 3 :  166 patients ( 16.6%) - Taux infarctus :  57.8%\n",
      "      Cluster 4 :  160 patients ( 16.0%) - Taux infarctus :  28.1%\n",
      "      Cluster 5 :  113 patients ( 11.3%) - Taux infarctus :  54.9%\n",
      "\n",
      "NOMBRE DE CLUSTERS : 7\n",
      "   Inertia               : 9427.12\n",
      "   Silhouette Score      : 7.43%\n",
      "   Davies-Bouldin Score  : 2.39\n",
      "\n",
      "   Distribution des patients par cluster :\n",
      "      Cluster 0 :  160 patients ( 16.0%) - Taux infarctus :   0.6%\n",
      "      Cluster 1 :  155 patients ( 15.5%) - Taux infarctus :  25.2%\n",
      "      Cluster 2 :  133 patients ( 13.3%) - Taux infarctus :  36.1%\n",
      "      Cluster 3 :  106 patients ( 10.6%) - Taux infarctus :  54.7%\n",
      "      Cluster 4 :  160 patients ( 16.0%) - Taux infarctus :   1.9%\n",
      "      Cluster 5 :  139 patients ( 13.9%) - Taux infarctus :  64.0%\n",
      "      Cluster 6 :  147 patients ( 14.7%) - Taux infarctus :   8.2%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MEILLEUR NOMBRE DE CLUSTERS : 2\n",
      "--------------------------------------------------------------------------------\n",
      "   Basé sur le Silhouette Score le plus eleve\n",
      "   Silhouette Score : 9.91%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pour K-Means, l'importance d'une variable est mesuree par sa contribution\n",
      "a la separation des clusters (variance inter-cluster / variance totale).\n",
      "\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   diabete                        : 1.475223\n",
      "   glycemie                       : 1.243441\n",
      "   imc                            : 0.491196\n",
      "   triglycerides                  : 0.394643\n",
      "   crp                            : 0.138362\n",
      "   frequence_cardiaque            : 0.013998\n",
      "   age                            : 0.010209\n",
      "   hypercholesterolemie           : 0.009998\n",
      "   tabagisme_1                    : 0.008795\n",
      "   ldl                            : 0.008218\n",
      "   sexe                           : 0.007419\n",
      "   hypertension                   : 0.005713\n",
      "   douleur_thoracique             : 0.005124\n",
      "   cholesterol_total              : 0.004395\n",
      "   pression_diastolique           : 0.003978\n",
      "   tabagisme_2                    : 0.003297\n",
      "   pression_systolique            : 0.002612\n",
      "   antecedents_familiaux          : 0.001454\n",
      "   sedentarite                    : 0.001447\n",
      "   hdl                            : 0.001164\n",
      "\n",
      "================================================================================\n",
      "K-MEANS (VERSION COMPLETE) TERMINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T05:18:39.424160400Z",
     "start_time": "2026-03-01T05:18:39.365318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "17. COMPARAISON DES MODELES SUPERVISES\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"17. COMPARAISON DES MODELES SUPERVISES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nRecapitulatif des performances de tous les modeles supervises sur l'ensemble de TEST :\")\n",
    "\n",
    "# Creer un tableau comparatif\n",
    "comparaison_modeles = []\n",
    "\n",
    "for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "    if nom_modele in resultats_modeles:\n",
    "        res = resultats_modeles[nom_modele]\n",
    "        comparaison_modeles.append({\n",
    "            'Modele': nom_modele,\n",
    "            'Precision globale (%)': f\"{res['accuracy_test']*100:.2f}\",\n",
    "            'Precision positive (%)': f\"{res['precision_test']*100:.2f}\",\n",
    "            'Recall (%)': f\"{res['recall_test']*100:.2f}\",\n",
    "            'F1-Score (%)': f\"{res['f1_test']*100:.2f}\",\n",
    "            'ROC-AUC (%)': f\"{res['roc_auc_test']*100:.2f}\"\n",
    "        })\n",
    "\n",
    "df_comparaison = pd.DataFrame(comparaison_modeles)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"TABLEAU COMPARATIF\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n\", df_comparaison.to_string(index=False))\n",
    "\n",
    "# Identifier le meilleur modele pour chaque metrique (en utilisant les valeurs numeriques)\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MEILLEURS MODELES PAR METRIQUE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "meilleurs = {}\n",
    "comparaison_numerique = []\n",
    "\n",
    "for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "    if nom_modele in resultats_modeles:\n",
    "        res = resultats_modeles[nom_modele]\n",
    "        comparaison_numerique.append({\n",
    "            'Modele': nom_modele,\n",
    "            'Precision globale': res['accuracy_test'],\n",
    "            'Precision positive': res['precision_test'],\n",
    "            'Recall': res['recall_test'],\n",
    "            'F1-Score': res['f1_test'],\n",
    "            'ROC-AUC': res['roc_auc_test']\n",
    "        })\n",
    "\n",
    "df_comparaison_num = pd.DataFrame(comparaison_numerique)\n",
    "\n",
    "for metrique in ['Precision globale', 'Precision positive', 'Recall', 'F1-Score', 'ROC-AUC']:\n",
    "    idx_max = df_comparaison_num[metrique].idxmax()\n",
    "    meilleur_modele = df_comparaison_num.loc[idx_max, 'Modele']\n",
    "    meilleure_valeur = df_comparaison_num.loc[idx_max, metrique]\n",
    "    meilleurs[metrique] = (meilleur_modele, meilleure_valeur)\n",
    "    print(f\"\\n{metrique:20s} : {meilleur_modele:25s} ({meilleure_valeur*100:.2f}%)\")\n",
    "\n",
    "# Modele recommande (basé sur F1-Score qui equilibre precision et recall)\n",
    "modele_recommande = meilleurs['F1-Score'][0]\n",
    "f1_recommande = meilleurs['F1-Score'][1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"MODELE RECOMMANDE : {modele_recommande}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCritere : F1-Score (equilibre entre precision positive et recall)\")\n",
    "print(f\"F1-Score : {f1_recommande*100:.2f}%\")\n",
    "print(f\"\\nCe modele offre le meilleur compromis pour predire le risque d'infarctus\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ANALYSE DU SURAPPRENTISSAGE (OVERFITTING)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "    if nom_modele in resultats_modeles:\n",
    "        res = resultats_modeles[nom_modele]\n",
    "        acc_train = res['accuracy_train']\n",
    "        acc_test = res['accuracy_test']\n",
    "        diff = acc_train - acc_test\n",
    "\n",
    "        print(f\"\\n{nom_modele:25s}\")\n",
    "        print(f\"   Precision globale Train : {acc_train*100:.2f}%\")\n",
    "        print(f\"   Precision globale Test  : {acc_test*100:.2f}%\")\n",
    "        print(f\"   Difference              : {diff*100:.2f}%\", end=\"\")\n",
    "\n",
    "        if diff < 0.05:\n",
    "            print(\" - Bon equilibre (pas de surapprentissage)\")\n",
    "        elif diff < 0.10:\n",
    "            print(\" - Leger surapprentissage\")\n",
    "        else:\n",
    "            print(\" - Surapprentissage important\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COMPARAISON DE L'IMPORTANCE DES VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Analyse des variables les plus importantes selon chaque modele.\n",
    "Cela permet d'identifier les facteurs de risque principaux d'infarctus.\n",
    "\"\"\")\n",
    "\n",
    "for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "    if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "        print(f\"\\n{nom_modele.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        feature_imp = resultats_modeles[nom_modele]['feature_importance']\n",
    "        print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "\n",
    "        for idx, row in feature_imp.head(20).iterrows():\n",
    "            print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VARIABLES COMMUNES DANS LE TOP 20 DE TOUS LES MODELES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Identifier les variables qui apparaissent dans le top 20 de tous les modeles\n",
    "top20_par_modele = {}\n",
    "for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "    if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "        feature_imp = resultats_modeles[nom_modele]['feature_importance']\n",
    "        top20_par_modele[nom_modele] = set(feature_imp.head(20)['feature'].tolist())\n",
    "\n",
    "# Trouver l'intersection\n",
    "if len(top20_par_modele) > 0:\n",
    "    variables_communes = set.intersection(*top20_par_modele.values())\n",
    "\n",
    "    if len(variables_communes) > 0:\n",
    "        print(f\"\\nVariables presentes dans le TOP 20 de tous les modeles ({len(variables_communes)}) :\")\n",
    "        print(\"\\nCe sont les facteurs de risque les PLUS CRITIQUES identifies par tous les modeles.\")\n",
    "        print(\"Classement par importance moyenne (sur les 4 modeles) :\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Calculer l'importance moyenne de chaque variable commune\n",
    "        importance_moyenne = {}\n",
    "        for var in variables_communes:\n",
    "            importances = []\n",
    "            for nom_modele in ['Regression Logistique', 'Random Forest', 'Gradient Boosting', 'SVM']:\n",
    "                if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "                    feature_imp = resultats_modeles[nom_modele]['feature_importance']\n",
    "                    importance_var = feature_imp[feature_imp['feature'] == var]['importance'].values\n",
    "                    if len(importance_var) > 0:\n",
    "                        importances.append(importance_var[0])\n",
    "\n",
    "            if len(importances) > 0:\n",
    "                importance_moyenne[var] = np.mean(importances)\n",
    "\n",
    "        # Trier par importance moyenne decroissante\n",
    "        variables_triees = sorted(importance_moyenne.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(f\"\\n{'Rang':>4s}  {'Variable':30s}  {'Importance moyenne':>18s}\")\n",
    "        print(\"-\" * 80)\n",
    "        for rang, (var, imp_moy) in enumerate(variables_triees, 1):\n",
    "            print(f\"{rang:4d}. {var:30s} : {imp_moy:.6f}\")\n",
    "    else:\n",
    "        print(\"\\nAucune variable commune dans le TOP 20 de tous les modeles.\")\n",
    "        print(\"\\nCela suggere que les modeles utilisent des strategies differentes\")\n",
    "        print(\"pour predire le risque d'infarctus.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARAISON DES MODELES SUPERVISES TERMINEE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "c7129656cc22998b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "17. COMPARAISON DES MODELES SUPERVISES\n",
      "================================================================================\n",
      "\n",
      "Recapitulatif des performances de tous les modeles supervises sur l'ensemble de TEST :\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TABLEAU COMPARATIF\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                Modele Precision globale (%) Precision positive (%) Recall (%) F1-Score (%) ROC-AUC (%)\n",
      "Regression Logistique                 97.00                  89.29     100.00        94.34       99.87\n",
      "        Random Forest                 89.50                  76.36      84.00        80.00       96.25\n",
      "    Gradient Boosting                 95.00                  93.48      86.00        89.58       98.28\n",
      "                  SVM                 91.50                  77.05      94.00        84.68       97.53\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MEILLEURS MODELES PAR METRIQUE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Precision globale    : Regression Logistique     (97.00%)\n",
      "\n",
      "Precision positive   : Gradient Boosting         (93.48%)\n",
      "\n",
      "Recall               : Regression Logistique     (100.00%)\n",
      "\n",
      "F1-Score             : Regression Logistique     (94.34%)\n",
      "\n",
      "ROC-AUC              : Regression Logistique     (99.87%)\n",
      "\n",
      "================================================================================\n",
      "MODELE RECOMMANDE : Regression Logistique\n",
      "================================================================================\n",
      "\n",
      "Critere : F1-Score (equilibre entre precision positive et recall)\n",
      "F1-Score : 94.34%\n",
      "\n",
      "Ce modele offre le meilleur compromis pour predire le risque d'infarctus\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANALYSE DU SURAPPRENTISSAGE (OVERFITTING)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Regression Logistique    \n",
      "   Precision globale Train : 96.38%\n",
      "   Precision globale Test  : 97.00%\n",
      "   Difference              : -0.62% - Bon equilibre (pas de surapprentissage)\n",
      "\n",
      "Random Forest            \n",
      "   Precision globale Train : 93.62%\n",
      "   Precision globale Test  : 89.50%\n",
      "   Difference              : 4.13% - Bon equilibre (pas de surapprentissage)\n",
      "\n",
      "Gradient Boosting        \n",
      "   Precision globale Train : 100.00%\n",
      "   Precision globale Test  : 95.00%\n",
      "   Difference              : 5.00% - Leger surapprentissage\n",
      "\n",
      "SVM                      \n",
      "   Precision globale Train : 96.50%\n",
      "   Precision globale Test  : 91.50%\n",
      "   Difference              : 5.00% - Bon equilibre (pas de surapprentissage)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARAISON DE L'IMPORTANCE DES VARIABLES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Analyse des variables les plus importantes selon chaque modele.\n",
      "Cela permet d'identifier les facteurs de risque principaux d'infarctus.\n",
      "\n",
      "\n",
      "REGRESSION LOGISTIQUE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   diabete                        : 0.164424\n",
      "   hypertension                   : 0.162368\n",
      "   douleur_thoracique             : 0.122520\n",
      "   dyspnee                        : 0.112230\n",
      "   sexe                           : 0.091279\n",
      "   tabagisme_2                    : 0.089227\n",
      "   hypercholesterolemie           : 0.074055\n",
      "   ldl                            : 0.049758\n",
      "   age                            : 0.044048\n",
      "   crp                            : 0.021494\n",
      "   glycemie                       : 0.018454\n",
      "   pression_systolique            : 0.010942\n",
      "   cholesterol_total              : 0.010860\n",
      "   antecedents_familiaux          : 0.005576\n",
      "   tabagisme_1                    : 0.004965\n",
      "   sedentarite                    : 0.004622\n",
      "   triglycerides                  : 0.004308\n",
      "   hdl                            : 0.002876\n",
      "   pression_diastolique           : 0.002873\n",
      "   frequence_cardiaque            : 0.002039\n",
      "\n",
      "RANDOM FOREST\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   diabete                        : 0.151663\n",
      "   hypertension                   : 0.149311\n",
      "   glycemie                       : 0.098893\n",
      "   ldl                            : 0.085419\n",
      "   age                            : 0.083628\n",
      "   cholesterol_total              : 0.074541\n",
      "   douleur_thoracique             : 0.072952\n",
      "   hypercholesterolemie           : 0.049647\n",
      "   pression_systolique            : 0.042944\n",
      "   imc                            : 0.037035\n",
      "   pression_diastolique           : 0.031681\n",
      "   crp                            : 0.029595\n",
      "   hdl                            : 0.019146\n",
      "   sexe                           : 0.015808\n",
      "   tabagisme_2                    : 0.015321\n",
      "   dyspnee                        : 0.014837\n",
      "   triglycerides                  : 0.014768\n",
      "   frequence_cardiaque            : 0.008952\n",
      "   sedentarite                    : 0.001894\n",
      "   antecedents_familiaux          : 0.001059\n",
      "\n",
      "GRADIENT BOOSTING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   diabete                        : 0.186343\n",
      "   hypertension                   : 0.174147\n",
      "   douleur_thoracique             : 0.099960\n",
      "   ldl                            : 0.089612\n",
      "   glycemie                       : 0.068641\n",
      "   cholesterol_total              : 0.065282\n",
      "   hypercholesterolemie           : 0.054892\n",
      "   age                            : 0.054804\n",
      "   sexe                           : 0.044499\n",
      "   crp                            : 0.042026\n",
      "   dyspnee                        : 0.038080\n",
      "   tabagisme_2                    : 0.035523\n",
      "   pression_systolique            : 0.015446\n",
      "   imc                            : 0.012071\n",
      "   pression_diastolique           : 0.009792\n",
      "   triglycerides                  : 0.003897\n",
      "   frequence_cardiaque            : 0.002783\n",
      "   hdl                            : 0.002204\n",
      "   antecedents_familiaux          : 0.000000\n",
      "   sedentarite                    : 0.000000\n",
      "\n",
      "SVM\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 20 des variables les plus importantes :\n",
      "   douleur_thoracique             : 0.052000\n",
      "   dyspnee                        : 0.044000\n",
      "   ldl                            : 0.042000\n",
      "   glycemie                       : 0.039500\n",
      "   age                            : 0.034500\n",
      "   sexe                           : 0.031500\n",
      "   diabete                        : 0.030500\n",
      "   hypercholesterolemie           : 0.026500\n",
      "   pression_systolique            : 0.026000\n",
      "   hypertension                   : 0.019000\n",
      "   cholesterol_total              : 0.018500\n",
      "   tabagisme_2                    : 0.014000\n",
      "   pression_diastolique           : 0.006000\n",
      "   frequence_cardiaque            : 0.005000\n",
      "   imc                            : 0.002000\n",
      "   hdl                            : 0.001500\n",
      "   triglycerides                  : -0.001000\n",
      "   antecedents_familiaux          : -0.001000\n",
      "   crp                            : -0.001500\n",
      "   tabagisme_1                    : -0.001500\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VARIABLES COMMUNES DANS LE TOP 20 DE TOUS LES MODELES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Variables presentes dans le TOP 20 de tous les modeles (18) :\n",
      "\n",
      "Ce sont les facteurs de risque les PLUS CRITIQUES identifies par tous les modeles.\n",
      "Classement par importance moyenne (sur les 4 modeles) :\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Rang  Variable                        Importance moyenne\n",
      "--------------------------------------------------------------------------------\n",
      "   1. diabete                        : 0.133232\n",
      "   2. hypertension                   : 0.126207\n",
      "   3. douleur_thoracique             : 0.086858\n",
      "   4. ldl                            : 0.066697\n",
      "   5. glycemie                       : 0.056372\n",
      "   6. age                            : 0.054245\n",
      "   7. dyspnee                        : 0.052287\n",
      "   8. hypercholesterolemie           : 0.051274\n",
      "   9. sexe                           : 0.045772\n",
      "  10. cholesterol_total              : 0.042295\n",
      "  11. tabagisme_2                    : 0.038518\n",
      "  12. pression_systolique            : 0.023833\n",
      "  13. crp                            : 0.022904\n",
      "  14. pression_diastolique           : 0.012587\n",
      "  15. hdl                            : 0.006431\n",
      "  16. triglycerides                  : 0.005493\n",
      "  17. frequence_cardiaque            : 0.004694\n",
      "  18. antecedents_familiaux          : 0.001409\n",
      "\n",
      "================================================================================\n",
      "COMPARAISON DES MODELES SUPERVISES TERMINEE\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "18. COMPARAISON DES MODELES NON SUPERVISES\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"18. COMPARAISON DES MODELES NON SUPERVISES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nRecapitulatif des resultats des modeles de clustering :\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"1. DISTANCE DE GOWER + CLUSTERING HIERARCHIQUE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'Gower + Hierarchical' in resultats_modeles:\n",
    "    res_gower = resultats_modeles['Gower + Hierarchical']\n",
    "    best_k = res_gower['best_n_clusters']\n",
    "    res_k = res_gower['resultats_par_k'][best_k]\n",
    "\n",
    "    print(f\"\\nNombre de clusters optimal : {best_k}\")\n",
    "    print(f\"Silhouette Score           : {res_k['silhouette']*100:.2f}%\")\n",
    "    print(f\"Davies-Bouldin Score       : {res_k['davies_bouldin']:.2f}\")\n",
    "\n",
    "    # Taux d'infarctus par cluster\n",
    "    print(f\"\\nTaux d'infarctus par cluster :\")\n",
    "    for cluster_id in range(1, best_k + 1):\n",
    "        cluster_mask = df_final['cluster_gower'] == cluster_id\n",
    "        n_patients = cluster_mask.sum()\n",
    "        n_infarctus = df_final.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux = (n_infarctus / n_patients * 100) if n_patients > 0 else 0\n",
    "        print(f\"   Cluster {cluster_id} : {n_patients:4d} patients - {taux:5.1f}% infarctus\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2. K-MEANS (VERSION NUMERIQUE)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'K-Means (numerique)' in resultats_modeles:\n",
    "    res_kmeans_num = resultats_modeles['K-Means (numerique)']\n",
    "    best_k = res_kmeans_num['best_n_clusters']\n",
    "    res_k = res_kmeans_num['resultats_par_k'][best_k]\n",
    "\n",
    "    print(f\"\\nNombre de clusters optimal : {best_k}\")\n",
    "    print(f\"Silhouette Score           : {res_k['silhouette']*100:.2f}%\")\n",
    "    print(f\"Davies-Bouldin Score       : {res_k['davies_bouldin']:.2f}\")\n",
    "    print(f\"Inertia                    : {res_k['inertia']:.2f}\")\n",
    "\n",
    "    # Taux d'infarctus par cluster\n",
    "    print(f\"\\nTaux d'infarctus par cluster :\")\n",
    "    for cluster_id in range(best_k):\n",
    "        cluster_mask = df_final['cluster_kmeans_num'] == cluster_id\n",
    "        n_patients = cluster_mask.sum()\n",
    "        n_infarctus = df_final.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux = (n_infarctus / n_patients * 100) if n_patients > 0 else 0\n",
    "        print(f\"   Cluster {cluster_id} : {n_patients:4d} patients - {taux:5.1f}% infarctus\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"3. K-MEANS (VERSION COMPLETE)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'K-Means (complete)' in resultats_modeles:\n",
    "    res_kmeans_comp = resultats_modeles['K-Means (complete)']\n",
    "    best_k = res_kmeans_comp['best_n_clusters']\n",
    "    res_k = res_kmeans_comp['resultats_par_k'][best_k]\n",
    "\n",
    "    print(f\"\\nNombre de clusters optimal : {best_k}\")\n",
    "    print(f\"Silhouette Score           : {res_k['silhouette']*100:.2f}%\")\n",
    "    print(f\"Davies-Bouldin Score       : {res_k['davies_bouldin']:.2f}\")\n",
    "    print(f\"Inertia                    : {res_k['inertia']:.2f}\")\n",
    "\n",
    "    # Taux d'infarctus par cluster\n",
    "    print(f\"\\nTaux d'infarctus par cluster :\")\n",
    "    for cluster_id in range(best_k):\n",
    "        cluster_mask = df_final['cluster_kmeans_complete'] == cluster_id\n",
    "        n_patients = cluster_mask.sum()\n",
    "        n_infarctus = df_final.loc[cluster_mask, 'infarctus'].sum()\n",
    "        taux = (n_infarctus / n_patients * 100) if n_patients > 0 else 0\n",
    "        print(f\"   Cluster {cluster_id} : {n_patients:4d} patients - {taux:5.1f}% infarctus\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"4. PCA (VARIANCE EXPLIQUEE)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'PCA' in resultats_modeles:\n",
    "    res_pca = resultats_modeles['PCA']\n",
    "    print(f\"\\nVariance expliquee par PC1 et PC2 : {res_pca['variance_expliquee_2d']:.4f} ({res_pca['variance_expliquee_2d']*100:.2f}%)\")\n",
    "    print(f\"Nombre de composantes pour 80% de variance : {res_pca['n_comp_80']}\")\n",
    "    print(f\"Nombre de composantes pour 95% de variance : {res_pca['n_comp_95']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COMPARAISON DES METHODES DE CLUSTERING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Comparer les Silhouette Scores\n",
    "comparaison_clustering = []\n",
    "\n",
    "if 'Gower + Hierarchical' in resultats_modeles:\n",
    "    res = resultats_modeles['Gower + Hierarchical']\n",
    "    k = res['best_n_clusters']\n",
    "    comparaison_clustering.append({\n",
    "        'Methode': 'Gower + Hierarchical',\n",
    "        'N_clusters': k,\n",
    "        'Silhouette (%)': f\"{res['resultats_par_k'][k]['silhouette']*100:.2f}\",\n",
    "        'Davies-Bouldin': f\"{res['resultats_par_k'][k]['davies_bouldin']:.2f}\"\n",
    "    })\n",
    "\n",
    "if 'K-Means (numerique)' in resultats_modeles:\n",
    "    res = resultats_modeles['K-Means (numerique)']\n",
    "    k = res['best_n_clusters']\n",
    "    comparaison_clustering.append({\n",
    "        'Methode': 'K-Means (numerique)',\n",
    "        'N_clusters': k,\n",
    "        'Silhouette (%)': f\"{res['resultats_par_k'][k]['silhouette']*100:.2f}\",\n",
    "        'Davies-Bouldin': f\"{res['resultats_par_k'][k]['davies_bouldin']:.2f}\"\n",
    "    })\n",
    "\n",
    "if 'K-Means (complete)' in resultats_modeles:\n",
    "    res = resultats_modeles['K-Means (complete)']\n",
    "    k = res['best_n_clusters']\n",
    "    comparaison_clustering.append({\n",
    "        'Methode': 'K-Means (complete)',\n",
    "        'N_clusters': k,\n",
    "        'Silhouette (%)': f\"{res['resultats_par_k'][k]['silhouette']*100:.2f}\",\n",
    "        'Davies-Bouldin': f\"{res['resultats_par_k'][k]['davies_bouldin']:.2f}\"\n",
    "    })\n",
    "\n",
    "df_clustering = pd.DataFrame(comparaison_clustering)\n",
    "print(\"\\n\", df_clustering.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"COMPARAISON DE L'IMPORTANCE DES VARIABLES (MODELES NON SUPERVISES)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "Analyse des variables les plus importantes selon chaque modele de clustering.\n",
    "Cela permet d'identifier les caracteristiques qui differencient le mieux les groupes de patients.\n",
    "\"\"\")\n",
    "\n",
    "for nom_modele in ['Gower + Hierarchical', 'K-Means (numerique)', 'K-Means (complete)', 'PCA']:\n",
    "    if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "        print(f\"\\n{nom_modele.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        feature_imp = resultats_modeles[nom_modele]['feature_importance']\n",
    "        print(\"\\nTop 20 des variables les plus importantes :\")\n",
    "\n",
    "        for idx, row in feature_imp.head(20).iterrows():\n",
    "            print(f\"   {row['feature']:30s} : {row['importance']:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VARIABLES COMMUNES DANS LE TOP 20 DE TOUS LES MODELES NON SUPERVISES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Identifier les variables qui apparaissent dans le top 20 de tous les modeles\n",
    "top20_par_modele_ns = {}\n",
    "for nom_modele in ['Gower + Hierarchical', 'K-Means (numerique)', 'K-Means (complete)', 'PCA']:\n",
    "    if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "        feature_imp = resultats_modeles[nom_modele]['feature_importance']\n",
    "        top20_par_modele_ns[nom_modele] = set(feature_imp.head(20)['feature'].tolist())\n",
    "\n",
    "# Trouver l'intersection\n",
    "if len(top20_par_modele_ns) > 0:\n",
    "    variables_communes_ns = set.intersection(*top20_par_modele_ns.values())\n",
    "\n",
    "    if len(variables_communes_ns) > 0:\n",
    "        print(f\"\\nVariables presentes dans le TOP 20 de tous les modeles non supervises ({len(variables_communes_ns)}) :\")\n",
    "        print(\"\\nCe sont les caracteristiques qui differencient le mieux les groupes de patients.\")\n",
    "        print(\"Classement par importance moyenne NORMALISEE (sur les 4 modeles) :\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # ETAPE 1 : Normaliser les importances de chaque modele (entre 0 et 1)\n",
    "        importances_normalisees_ns = {}\n",
    "\n",
    "        for nom_modele in ['Gower + Hierarchical', 'K-Means (numerique)', 'K-Means (complete)', 'PCA']:\n",
    "            if nom_modele in resultats_modeles and 'feature_importance' in resultats_modeles[nom_modele]:\n",
    "                feature_imp = resultats_modeles[nom_modele]['feature_importance'].copy()\n",
    "\n",
    "                # Normaliser : (valeur - min) / (max - min)\n",
    "                min_imp = feature_imp['importance'].min()\n",
    "                max_imp = feature_imp['importance'].max()\n",
    "\n",
    "                if max_imp > min_imp:\n",
    "                    feature_imp['importance_norm'] = (feature_imp['importance'] - min_imp) / (max_imp - min_imp)\n",
    "                else:\n",
    "                    feature_imp['importance_norm'] = 0.0\n",
    "\n",
    "                importances_normalisees_ns[nom_modele] = feature_imp\n",
    "\n",
    "        # ETAPE 2 : Calculer l'importance moyenne normalisee pour chaque variable commune\n",
    "        importance_moyenne_ns = {}\n",
    "        for var in variables_communes_ns:\n",
    "            importances_norm = []\n",
    "            for nom_modele in ['Gower + Hierarchical', 'K-Means (numerique)', 'K-Means (complete)', 'PCA']:\n",
    "                if nom_modele in importances_normalisees_ns:\n",
    "                    feature_imp = importances_normalisees_ns[nom_modele]\n",
    "                    importance_var = feature_imp[feature_imp['feature'] == var]['importance_norm'].values\n",
    "                    if len(importance_var) > 0:\n",
    "                        importances_norm.append(importance_var[0])\n",
    "\n",
    "            if len(importances_norm) > 0:\n",
    "                importance_moyenne_ns[var] = np.mean(importances_norm)\n",
    "\n",
    "        # Trier par importance moyenne decroissante\n",
    "        variables_triees_ns = sorted(importance_moyenne_ns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(f\"\\n{'Rang':>4s}  {'Variable':30s}  {'Importance moyenne':>18s}\")\n",
    "        print(\"-\" * 80)\n",
    "        for rang, (var, imp_moy) in enumerate(variables_triees_ns, 1):\n",
    "            print(f\"{rang:4d}. {var:30s} : {imp_moy:.6f}\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"NOTE : Importances normalisees entre 0 et 1 pour chaque modele avant calcul de la moyenne\")\n",
    "        print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"\\nAucune variable commune dans le TOP 20 de tous les modeles non supervises.\")\n",
    "        print(\"\\nCela suggere que les modeles utilisent des strategies differentes\")\n",
    "        print(\"pour regrouper les patients.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARAISON DES MODELES NON SUPERVISES TERMINEE\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "a3897b86e7d8d2ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
